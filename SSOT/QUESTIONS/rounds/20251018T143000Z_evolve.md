# Question Evolution - Round 20251018T143000Z

## Analysis Context
- Source: AGENT.md (Marketing Brand Intelligence - AI Agent Architecture v2.0)
- Components: 50+ agents across 6 tiers
- Focus: Production-critical gaps in RAG-only LLM, privacy-safe identity, MMM/MTA attribution

## High-Leverage Questions Generated

### Q_021: JSON Schema Validation Implementation
**Hypothesis**: All LLM outputs must validate against strict JSON schemas before use
**Why it matters**: Prevents malformed data from entering SSOT; critical for RAG-only principle
**Expected impact**: Zero hallucination leakage into decision systems
**How to verify**: 
- Create unit tests with invalid JSON inputs
- Verify schema enforcement at LLMGuardrails level
- Check all 8 LLM integration points have schema validation
**Proposed change**: 
```python
# Add to LLMGuardrails
@staticmethod
def validate_output(output: str, schema: Type[BaseModel]) -> ValidationResult:
    try:
        parsed = schema.parse_raw(output)
    except ValidationError as e:
        return ValidationResult(valid=False, error='schema_violation')
    return ValidationResult(valid=True, data=parsed)
```
**Targets**: ["LLMGuardrails.validate_llm_output", "all_llm_integration_points"]
**Urgency**: P0
**Status**: open

### Q_022: Source Citation Requirement Enforcement
**Hypothesis**: Every LLM factual claim must cite >=2 source_ids from provided context
**Why it matters**: Core RAG-only principle; prevents LLM from using training data
**Expected impact**: Eliminates invented facts in crisis briefs and creative variants
**How to verify**:
- Test LLM call with 0 sources → must fail
- Test LLM call with 1 source → must fail  
- Test with 2+ sources → must succeed with citations
**Proposed change**:
```python
# In LLMGuardrails.validate_llm_output
if not parsed.source_ids or len(parsed.source_ids) < 2:
    return ValidationResult(valid=False, error='insufficient_sources')

for source_id in parsed.source_ids:
    if source_id not in [s.id for s in sources]:
        return ValidationResult(valid=False, error='invalid_source_id')
```
**Targets**: ["LLMGuardrails.validate_llm_output", "CrisisDetectionAgent", "CreativeIntelligenceAgent", "BriefingAgent"]
**Urgency**: P0
**Status**: open

### Q_023: Verifier Model Separation Enforcement
**Hypothesis**: Verifier LLM must be different model from Analyst LLM
**Why it matters**: Same model cannot effectively verify its own outputs
**Expected impact**: Catches hallucinations and policy violations
**How to verify**:
- Check ModelRegistry.yaml has separate verifier config
- Verify CrisisDetectionAgent uses different model for verify_crisis
- Test: force same model → must fail with config error
**Proposed change**:
```yaml
# ModelRegistry.yaml enforcement
managed_main:
  tasks: [summary, creative, complex_reasoning]
  models: ["anthropic:claude-sonnet-4.5"]
  
managed_verifier:
  tasks: [policy_check, factual_verify]
  models: ["anthropic:claude-opus-4"]  # MUST be different
  
routing_validation:
  enforce_verifier_separation: true
  fail_if_same_model: true
```
**Targets**: ["ModelRegistry.yaml", "LLMGuardrails", "CrisisDetectionAgent.verify_crisis"]
**Urgency**: P0
**Status**: open

### Q_024: PII Hash Salt Management & Rotation
**Hypothesis**: Salt must be configurable, rotatable, and never logged
**Why it matters**: GDPR compliance; static salt is security vulnerability
**Expected impact**: Enables compliance with data retention policies
**How to verify**:
- Check SALT not in code/logs
- Test salt rotation: hash before/after must differ
- Verify user_key remains stable within salt generation
**Proposed change**:
```python
class SecurityLayer:
    def __init__(self):
        self.salt = self._load_salt_from_secret_manager()
    
    def _load_salt_from_secret_manager(self) -> bytes:
        # GCP Secret Manager or AWS Secrets Manager
        secret_name = "mbi/pii-salt-current"
        return secret_manager.get_secret(secret_name)
    
    def rotate_salt(self, new_salt: bytes):
        # Atomic rotation with migration plan
        old_salt = self.salt
        self.salt = new_salt
        # Trigger background re-hash job for existing users
```
**Targets**: ["SecurityLayer.hash_pii", "IdentityResolutionAgent", "PrivacyHasher"]
**Urgency**: P0
**Status**: open

### Q_025: Feature Store Online/Offline Parity Tests
**Hypothesis**: Features computed online must match offline for same input
**Why it matters**: Parity violations cause train/serve skew and model drift
**Expected impact**: Maintain ML model accuracy in production
**How to verify**:
- Generate test events
- Compute features online (real-time) and offline (batch)
- Assert feature values match within tolerance
**Proposed change**:
```python
# Add to tests/test_feature_store.py
def test_online_offline_parity():
    event = generate_test_event()
    
    # Online path
    online_features = feature_store.get_online_features(
        entity_id=event.user_key,
        feature_names=['ltv_30d', 'avg_order_value']
    )
    
    # Offline path (same data)
    offline_features = feature_store.get_offline_features(
        entity_id=event.user_key,
        feature_names=['ltv_30d', 'avg_order_value'],
        timestamp=event.timestamp
    )
    
    assert_features_equal(online_features, offline_features, tolerance=0.01)
```
**Targets**: ["FeatureStore", "FeatureEngineeringAgent", "tests/test_feature_store.py"]
**Urgency**: P1
**Status**: open

### Q_026: Idempotency Keys for Budget Allocation
**Hypothesis**: Budget changes must be idempotent with allocation_id
**Why it matters**: Re-runs or retries must not double-apply budget changes
**Expected impact**: Prevents overspend and budget corruption
**How to verify**:
- Execute allocation with ID=A123
- Re-execute same allocation ID → must skip
- Check audit log shows only one application
**Proposed change**:
```python
class BudgetAllocationAgent:
    async def execute_allocation(self, allocation: BudgetAllocation):
        # Check if already applied
        existing = await self.audit_log.get(allocation.allocation_id)
        if existing and existing.status == 'applied':
            return AllocationResult(status='already_applied', skipped=True)
        
        # Apply atomically with ID
        async with self.transaction():
            await self.apply_changes(allocation)
            await self.audit_log.record(
                allocation_id=allocation.allocation_id,
                status='applied',
                timestamp=datetime.utcnow()
            )
```
**Targets**: ["BudgetAllocationAgent", "ActivationAgent.push_to_ad_apis", "audit_log"]
**Urgency**: P0
**Status**: open

### Q_027: Event Deduplication with event_id
**Hypothesis**: Kafka consumers must deduplicate events using event_id
**Why it matters**: Kafka retries cause duplicate processing; breaks aggregations
**Expected impact**: Accurate metrics and decision quality
**How to verify**:
- Send same event_id twice
- Verify only one insertion in database
- Check idempotency window (7 days per spec)
**Proposed change**:
```python
@event_handler('spend_record_ingested')
async def on_spend_ingested(event):
    # Check idempotency cache
    cache_key = f"event:{event.event_id}"
    if await redis.exists(cache_key):
        logger.info(f"Duplicate event {event.event_id}, skipping")
        return
    
    # Process event
    await process_spend_record(event)
    
    # Mark as processed (TTL 7 days)
    await redis.setex(cache_key, 7 * 86400, '1')
```
**Targets**: ["KafkaMessageBus", "event_handlers", "IngestAgents"]
**Urgency**: P0
**Status**: open

### Q_028: MMM Seasonality & Holiday Controls
**Hypothesis**: MMM model must include holiday calendar and seasonality controls
**Why it matters**: Missing controls bias channel contribution estimates
**Expected impact**: More accurate ROI curves for budget optimization
**How to verify**:
- Train MMM with/without holiday controls
- Compare coefficient stability
- Validate against known seasonal campaigns (Black Friday)
**Proposed change**:
```python
# Add to MMMAgent.train_model
holiday_calendar = self._get_holiday_calendar(country='JP')
seasonality = self._create_fourier_features(period=365.25, order=3)

with pm.Model() as model:
    # ... existing priors ...
    
    # Holiday effects
    holiday_coef = pm.Normal('holiday_coef', mu=0, sigma=1)
    holiday_effect = holiday_coef * holiday_calendar
    
    # Seasonality
    season_coef = pm.Normal('season_coef', mu=0, sigma=0.5, shape=6)
    seasonal_effect = season_coef @ seasonality.T
    
    # Revenue model
    mu = (base_revenue + 
          channel_contributions +
          holiday_effect +
          seasonal_effect)
```
**Targets**: ["MMMAgent.train_model", "holiday_calendar", "controls"]
**Urgency**: P1
**Status**: open

### Q_029: MTA Path Aggregation Privacy
**Hypothesis**: Conversion paths must aggregate to bands with min_users_per_path
**Why it matters**: GDPR/privacy requires no individual-level path tracking
**Expected impact**: Compliant MTA without sacrificing accuracy
**How to verify**:
- Check all ConversionPath have conversions >= min_users (e.g., 10)
- Test with rare path → must aggregate into broader band
**Proposed change**:
```python
class MTAAgent:
    MIN_USERS_PER_PATH = 10  # Privacy threshold
    
    async def build_paths(self, lookback_days: int):
        raw_paths = await self._extract_raw_paths(lookback_days)
        
        # Aggregate to privacy-safe bands
        aggregated = defaultdict(lambda: {'conversions': 0, 'revenue': 0})
        for path in raw_paths:
            band_id = self._path_to_band(path)  # Group similar paths
            aggregated[band_id]['conversions'] += 1
            aggregated[band_id]['revenue'] += path.revenue
        
        # Filter by minimum users
        return [
            ConversionPath(
                path_id=band_id,
                conversions=stats['conversions'],
                revenue=stats['revenue']
            )
            for band_id, stats in aggregated.items()
            if stats['conversions'] >= self.MIN_USERS_PER_PATH
        ]
```
**Targets**: ["MTAAgent._compute_transitions", "ConversionPath", "privacy_policy"]
**Urgency**: P0
**Status**: open

### Q_030: Schema Drift Detection & Alerting
**Hypothesis**: Schema changes in upstream sources must trigger alerts before breaking pipelines
**Why it matters**: Prevents silent data quality failures
**Expected impact**: Faster incident response
**How to verify**:
- Change BigQuery schema (add/remove column)
- Verify SchemaValidatorAgent detects drift
- Check alert sent to monitoring
**Proposed change**:
```python
class SchemaValidatorAgent:
    async def validate_schema(self, table: str, expected_schema: Dict):
        current_schema = await self.get_current_schema(table)
        
        drift = self._compute_drift(expected_schema, current_schema)
        
        if drift.has_breaking_changes:
            await self.alert_agent.send_alert(
                severity='P1',
                message=f'Breaking schema change detected in {table}',
                details=drift.changes
            )
            raise SchemaBreakingChange(drift)
        
        if drift.has_additions:
            logger.warning(f'Schema additions in {table}: {drift.additions}')
```
**Targets**: ["SchemaValidatorAgent", "DataQualityAgent", "dbt_tests"]
**Urgency**: P1
**Status**: open

### Q_031: Playbook Approval Timeout & Fallback
**Hypothesis**: Approvals stuck >24h must auto-escalate or reject
**Why it matters**: Prevents system gridlock from human bottlenecks
**Expected impact**: Maintain automation velocity
**How to verify**:
- Request approval, wait 24h
- Verify timeout triggers fallback action
- Check escalation notification sent
**Proposed change**:
```python
class ApprovalAgent:
    async def request_approval(self, decision, approvers, timeout_hours=24):
        approval_id = str(uuid.uuid4())
        
        # Store with expiry
        await self.approvals_db.create(
            id=approval_id,
            decision=decision,
            approvers=approvers,
            expires_at=datetime.utcnow() + timedelta(hours=timeout_hours),
            status='pending'
        )
        
        # Schedule timeout check
        await self.scheduler.schedule(
            func=self._handle_timeout,
            args=[approval_id],
            run_at=datetime.utcnow() + timedelta(hours=timeout_hours)
        )
    
    async def _handle_timeout(self, approval_id):
        approval = await self.approvals_db.get(approval_id)
        if approval.status == 'pending':
            # Auto-reject or escalate
            await self.reject_with_escalation(approval)
```
**Targets**: ["ApprovalAgent", "PlaybookOrchestrator", "timeout_handling"]
**Urgency**: P1
**Status**: open

### Q_032: Promo Label Hard Enforcement
**Hypothesis**: Creative variants without Promo/広告 label must be hard-rejected
**Why it matters**: Legal compliance requirement for advertising
**Expected impact**: Zero regulatory violations
**How to verify**:
- Generate ad text without label
- Verify ComplianceAgent.check_compliance returns approved=False
- Test cannot proceed to activation
**Proposed change**:
```python
class ComplianceAgent:
    REQUIRED_LABELS = {
        'ja': ['広告', 'プロモーション', 'PR'],
        'en': ['Promo', 'Ad', 'Sponsored'],
        'vi': ['Quảng cáo', 'Promo'],
        'zh': ['广告', '推广']
    }
    
    async def check_compliance(self, text: str, language: str):
        # Hard check for promo label
        labels = self.REQUIRED_LABELS.get(language, [])
        has_label = any(label in text for label in labels)
        
        if not has_label:
            return ComplianceResult(
                approved=False,
                violations=['MISSING_PROMO_LABEL'],
                message=f'Required label missing for {language} ad'
            )
```
**Targets**: ["ComplianceAgent.check_compliance", "CreativeVariantsAgent", "policy_gates"]
**Urgency**: P0
**Status**: open

### Q_033: Lock Cleanup Job for Orphaned Locks
**Hypothesis**: Locks expired >30min must be auto-cleaned
**Why it matters**: Process crashes leave orphaned locks blocking execution
**Expected impact**: System self-healing
**How to verify**:
- Create lock, kill process
- Wait 30min
- Verify lock cleanup job removes it
**Proposed change**:
```python
# Add scheduled job
@scheduled('*/10 * * * *')  # Every 10 minutes
async def cleanup_expired_locks():
    now = datetime.utcnow()
    
    # Find all locks
    locks = await redis.keys('*.lock.json')
    
    for lock_key in locks:
        lock_data = await redis.get(lock_key)
        lock = json.loads(lock_data)
        
        started_at = datetime.fromisoformat(lock['started_at_utc'])
        expires_in = lock.get('expires_in_minutes', 30)
        
        if now > started_at + timedelta(minutes=expires_in):
            await redis.delete(lock_key)
            logger.warning(f'Cleaned expired lock: {lock_key}')
```
**Targets**: ["lock_management", "scheduled_jobs", "redis_locks"]
**Urgency**: P1
**Status**: open

### Q_034: Attribution Reconciliation Report
**Hypothesis**: MMM and MTA must produce weekly reconciliation showing differences
**Why it matters**: Divergence >20% indicates measurement issues
**Expected impact**: Trustworthy attribution for budget decisions
**How to verify**:
- Run MMM and MTA on same week
- Generate reconciliation report
- Flag channels where difference >20%
**Proposed change**:
```python
class AttributionReconciliationAgent:
    async def generate_weekly_report(self, week_start: date):
        mmm_attr = await self.mmm_agent.get_attribution(week_start)
        mta_attr = await self.mta_agent.get_attribution(week_start)
        
        report = {
            'week': week_start,
            'channels': [],
            'overall_alignment': 0.0
        }
        
        for channel in mmm_attr.keys():
            mmm_pct = mmm_attr[channel]
            mta_pct = mta_attr.get(channel, 0)
            diff_pct = abs(mmm_pct - mta_pct)
            
            report['channels'].append({
                'channel': channel,
                'mmm': mmm_pct,
                'mta': mta_pct,
                'difference': diff_pct,
                'flag': 'MAJOR_DIVERGENCE' if diff_pct > 0.20 else 'OK'
            })
        
        await self.save_report(report)
```
**Targets**: ["AttributionReconciliationAgent", "MMMAgent", "MTAAgent", "weekly_reports"]
**Urgency**: P2
**Status**: open

### Q_035: Data Freshness SLA Monitoring
**Hypothesis**: Ingestion lag >6h must trigger P1 alert
**Why it matters**: Stale data degrades decision quality
**Expected impact**: Maintain real-time intelligence
**How to verify**:
- Simulate ingestion delay
- Verify alert fires at 6h threshold
- Check dashboard shows freshness status
**Proposed change**:
```python
class DataQualityAgent:
    FRESHNESS_SLA_HOURS = 6
    
    async def check_freshness(self):
        sources = ['meta_ads', 'google_ads', 'ga4', 'shopify']
        
        for source in sources:
            latest_timestamp = await self.get_latest_ingestion(source)
            age_hours = (datetime.utcnow() - latest_timestamp).total_seconds() / 3600
            
            if age_hours > self.FRESHNESS_SLA_HOURS:
                await self.alert_agent.send_alert(
                    severity='P1',
                    message=f'{source} data is {age_hours:.1f}h old (SLA: {self.FRESHNESS_SLA_HOURS}h)',
                    dashboard_link='...'
                )
```
**Targets**: ["DataQualityAgent.check_freshness", "MonitoringAgent", "freshness_sla"]
**Urgency**: P1
**Status**: open

### Q_036: LLM Temperature Validator
**Hypothesis**: All LLM calls must enforce temperature <=0.2
**Why it matters**: Higher temperature reduces determinism
**Expected impact**: Reproducible LLM outputs
**How to verify**:
- Attempt LLM call with temperature=0.5
- Verify config validation fails
- Check all calls in codebase respect limit
**Proposed change**:
```python
class LLMGuardrails:
    MAX_TEMPERATURE = 0.2
    
    async def call_llm(self, prompt: str, temperature: float, **kwargs):
        if temperature > self.MAX_TEMPERATURE:
            raise ConfigurationError(
                f'Temperature {temperature} exceeds max {self.MAX_TEMPERATURE}'
            )
        
        # Enforce in API call
        response = await self.llm_api.complete(
            prompt=prompt,
            temperature=min(temperature, self.MAX_TEMPERATURE),
            **kwargs
        )
```
**Targets**: ["LLMGuardrails.call_llm", "all_llm_integration_points", "config_validation"]
**Urgency**: P0
**Status**: open

### Q_037: Crisis Escalation PagerDuty Integration
**Hypothesis**: Crisis risk_score >=0.8 must page on-call
**Why it matters**: High-risk crises need immediate human response
**Expected impact**: Faster crisis mitigation
**How to verify**:
- Trigger crisis with risk_score=0.85
- Verify PagerDuty incident created
- Check escalation policy activated
**Proposed change**:
```python
class CrisisDetectionAgent:
    CRITICAL_RISK_THRESHOLD = 0.8
    
    async def verify_crisis(self, ...):
        verified = await self.verifier_llm.verify_crisis(...)
        
        if verified.risk_score >= self.CRITICAL_RISK_THRESHOLD:
            # Create PagerDuty incident
            await self.pagerduty_client.create_incident(
                title=f'Brand Crisis Detected: {verified.topic_id}',
                description=verified.reasons,
                urgency='high',
                service_id=config.CRISIS_SERVICE_ID
            )
            
            # Also notify Slack
            await self.alert_agent.send_alert(...)
```
**Targets**: ["CrisisDetectionAgent.verify_crisis", "AlertAgent", "pagerduty_integration"]
**Urgency**: P1
**Status**: open

### Q_038: Audience Expansion Performance Gates
**Hypothesis**: Test audiences with CPA >target_cpa*1.5 must auto-pause
**Why it matters**: Prevents budget waste on underperforming segments
**Expected impact**: Improved ROAS
**How to verify**:
- Launch test audience
- Simulate CPA exceeding threshold
- Verify auto-pause within 24h
**Proposed change**:
```python
class AudienceExpansionAgent:
    async def monitor_test_audiences(self):
        active_tests = await self.get_active_tests()
        
        for test in active_tests:
            metrics = await self.fetch_metrics(test.audience_id)
            
            if metrics.cpa > test.target_cpa * 1.5:
                await self.pause_audience(
                    test.audience_id,
                    reason='CPA_THRESHOLD_EXCEEDED'
                )
                
                await self.notify_team(
                    f'Auto-paused {test.audience_id}: CPA ${metrics.cpa} vs target ${test.target_cpa}'
                )
```
**Targets**: ["AudienceExpansionAgent.monitor_tests", "performance_gates", "auto_pause"]
**Urgency**: P1
**Status**: open

### Q_039: Bayesian A/B Test Early Stopping
**Hypothesis**: Tests with P(best)>0.95 can stop early
**Why it matters**: Reduces test duration and accelerates learning
**Expected impact**: Faster iteration cycles
**How to verify**:
- Simulate A/B test with clear winner
- Verify stops when confidence threshold met
- Check minimum sample size still respected
**Proposed change**:
```python
class BayesianABTest:
    MIN_SAMPLES_PER_VARIANT = 1000
    CONFIDENCE_THRESHOLD = 0.95
    
    async def check_early_stopping(self, test_id):
        test = await self.get_test(test_id)
        
        # Minimum sample requirement
        if any(v.impressions < self.MIN_SAMPLES_PER_VARIANT 
               for v in test.variants):
            return False
        
        # Bayesian check
        posteriors = [self.update_posterior(v) for v in test.variants]
        probs = self.probability_best(posteriors)
        
        max_prob = max(probs.values())
        if max_prob > self.CONFIDENCE_THRESHOLD:
            winner_id = max(probs, key=probs.get)
            await self.stop_test(test_id, winner_id, reason='early_stopping')
            return True
```
**Targets**: ["BayesianABTest.check_early_stopping", "ABTestAgent", "early_stopping_logic"]
**Urgency**: P2
**Status**: open

### Q_040: BigQuery Partition Pruning Optimization
**Hypothesis**: Date-partitioned queries must use partition filters
**Why it matters**: Reduces query costs by 10-100x
**Expected impact**: Lower cloud spend
**How to verify**:
- Run query without partition filter
- Check query plan shows full scan
- Add filter, verify only relevant partitions scanned
**Proposed change**:
```python
# Add to all BigQuery queries
class BigQueryClient:
    def query(self, sql: str, params: Dict):
        # Validate partition filter present
        if 'fct_' in sql and 'WHERE' in sql:
            if not any(date_col in sql for date_col in ['dt', 'date', 'order_date']):
                raise QueryOptimizationError(
                    'Partitioned table query missing partition filter'
                )
        
        return self.client.query(sql, params)
```
**Targets**: ["BigQueryClient", "all_analytical_queries", "cost_optimization"]
**Urgency**: P2
**Status**: open

## Summary
- Generated: 20 new high-leverage questions (Q_021 to Q_040)
- Total open questions: 40 (previous 20 + new 20)
- P0 (critical): 12 questions
- P1 (high): 7 questions  
- P2 (medium): 1 question
- Primary themes:
  1. LLM guardrails & RAG-only enforcement (8 questions)
  2. Privacy & compliance (5 questions)
  3. Data quality & observability (5 questions)
  4. Idempotency & system reliability (4 questions)
  5. Attribution accuracy (2 questions)
  6. Performance optimization (2 questions)

## Next Actions
1. Reconcile into INTENT_CAPSULE.md (maintain ≤1.5KB)
2. Prioritize P0 questions for Phase 4 (EXECUTE)
3. Update phase_state.json: next_expected_phase="AUDIT"
