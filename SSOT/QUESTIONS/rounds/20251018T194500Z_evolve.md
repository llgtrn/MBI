{
  "questions": [
    {
      "id": "Q_101",
      "area": "Data",
      "question": "How do we enforce MIN_RECORDS=100 per aggregate before publishing to Feature Store?",
      "why_it_matters": "Prevents statistical noise from small samples; Feature Store parity check (A_008) requires stable distributions",
      "evidence_anchor": ["core/feature_store.py", "agents/feature_engineering.py"],
      "acceptance_signal": "Test with 50 records rejected; 100+ accepted with logged counts",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_102",
      "area": "Secrets",
      "question": "What is the TTL and rotation schedule for PII hashing salt in Secret Manager?",
      "why_it_matters": "Q_070 requires salt rotation; GDPR mandates periodic re-keying; no current rotation config visible",
      "evidence_anchor": ["core/security.py", "infrastructure/secret_manager.yaml"],
      "acceptance_signal": "Config shows TTL=90d with automated rotation policy; test rotation loads new salt",
      "urgency": "P0",
      "owner_hint": "Security"
    },
    {
      "id": "Q_103",
      "area": "Execution",
      "question": "How do we prevent budget mutations when daily_budget exceeds campaign.max_budget?",
      "why_it_matters": "Q_082 requires budget limit validation; prevents overspend; no constraint visible in activation logic",
      "evidence_anchor": ["agents/activation_agent.py", "agents/budget_allocation.py"],
      "acceptance_signal": "Submit allocation exceeding max_budget; verify HTTP 400 with budget_limit_exceeded error",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_104",
      "area": "Observability",
      "question": "Are OpenTelemetry spans tagged with agent_name, operation_type, and status for all agent calls?",
      "why_it_matters": "Q_078 requires E2E tracing; current spans lack semantic tags for filtering/aggregation in Jaeger",
      "evidence_anchor": ["core/tracing.py", "agents/base_agent.py"],
      "acceptance_signal": "Query Jaeger for agent_name=MMMAgent; verify spans include operation_type and exit_status tags",
      "urgency": "P0",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_105",
      "area": "Compliance",
      "question": "How do we enforce age gate for <18 users in audience targeting API?",
      "why_it_matters": "Q_097 requires age gate; regulatory compliance JP/VN; no age filter in targeting validation",
      "evidence_anchor": ["agents/compliance_agent.py", "agents/audience_expansion.py"],
      "acceptance_signal": "Submit audience with age_range=[13,17]; verify rejection with age_gate_violation error",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_106",
      "area": "Risk",
      "question": "What is the HMAC secret rotation policy for webhook receivers?",
      "why_it_matters": "Q_098 requires HMAC validation; webhook spoofing prevention; no rotation schedule visible",
      "evidence_anchor": ["agents/webhook_receiver.py", "infrastructure/secret_manager.yaml"],
      "acceptance_signal": "Config shows HMAC secret rotation policy; test invalid HMAC returns HTTP 401",
      "urgency": "P0",
      "owner_hint": "Security"
    },
    {
      "id": "Q_107",
      "area": "Observability",
      "question": "How do P0 alerts trigger PagerDuty within 2min SLA?",
      "why_it_matters": "Q_100 requires incident response; current alertmanager may not have P0 routing configured",
      "evidence_anchor": ["infrastructure/alertmanager.yaml", "infrastructure/pagerduty.yaml"],
      "acceptance_signal": "Simulate P0 error; verify PagerDuty page in <2min with incident correlation_id",
      "urgency": "P0",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_108",
      "area": "Data",
      "question": "How do we enforce event_id deduplication with Redis TTL=7d for Event Bus?",
      "why_it_matters": "Q_058 and A_005 require idempotency; Kafka retries cause duplicates; no dedup logic in message_bus",
      "evidence_anchor": ["core/message_bus.py", "core/event.py"],
      "acceptance_signal": "Send duplicate event_id; verify second event skipped with duplicate_event logged",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_109",
      "area": "Governance",
      "question": "How do we enforce schema version compatibility when deserializing events?",
      "why_it_matters": "Q_071 requires backward compat; schema drift causes deserialization failures; no versioning in contracts",
      "evidence_anchor": ["core/contracts.py", "core/event.py"],
      "acceptance_signal": "Deserialize v1 event with v2 schema; verify success with field migration logged",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_110",
      "area": "RAG",
      "question": "How do we enforce temperature<=0.2 for all LLM calls via guardrails?",
      "why_it_matters": "Q_072 requires determinism; RAG-only principle; current LLM calls may not validate temperature",
      "evidence_anchor": ["agents/llm_guardrails.py", "agents/crisis_detection.py"],
      "acceptance_signal": "Submit LLM request with temperature=0.5; verify rejection with temperature_violation error",
      "urgency": "P0",
      "owner_hint": "LLM"
    },
    {
      "id": "Q_111",
      "area": "RAG",
      "question": "How do we enforce source_ids>=2 for all LLM outputs via guardrails?",
      "why_it_matters": "Q_073 requires RAG-only principle; prevents hallucination; no validation in LLM response parsing",
      "evidence_anchor": ["agents/llm_guardrails.py", "agents/briefing_agent.py"],
      "acceptance_signal": "Parse LLM output with 1 source; verify rejection with insufficient_sources error",
      "urgency": "P0",
      "owner_hint": "LLM"
    },
    {
      "id": "Q_112",
      "area": "RAG",
      "question": "How do we enforce analyst_model != verifier_model in model registry config?",
      "why_it_matters": "Q_074 requires verifier separation; self-verification impossible; no validation in model routing",
      "evidence_anchor": ["config/model_registry.yaml", "agents/llm_council.py"],
      "acceptance_signal": "Configure analyst=verifier; verify startup error with model_separation_violation",
      "urgency": "P0",
      "owner_hint": "LLM"
    },
    {
      "id": "Q_113",
      "area": "RBAC",
      "question": "How do we enforce JWT token validation and return HTTP 401 for missing/invalid tokens?",
      "why_it_matters": "Q_075 requires RBAC enforcement; unauthorized access prevention; no JWT middleware visible in API",
      "evidence_anchor": ["infrastructure/api_gateway.py", "core/auth.py"],
      "acceptance_signal": "Call API without token; verify HTTP 401 with unauthorized error; verify valid token succeeds",
      "urgency": "P0",
      "owner_hint": "Security"
    },
    {
      "id": "Q_114",
      "area": "Compliance",
      "question": "How do we enforce >=2 independent sources for crisis detection before triggering alerts?",
      "why_it_matters": "Q_088 requires false positive prevention; single-source crises may be unreliable; no multi-source check",
      "evidence_anchor": ["agents/crisis_detection.py", "agents/llm_council.py"],
      "acceptance_signal": "Submit crisis with 1 source; verify requires_human_review=true with verify_official action",
      "urgency": "P0",
      "owner_hint": "Brand"
    },
    {
      "id": "Q_115",
      "area": "Secrets",
      "question": "How do we enforce all secrets loaded from Secret Manager and audit for hardcoded secrets?",
      "why_it_matters": "Q_090 requires secret hygiene; env var leaks; no audit script visible in CI/CD",
      "evidence_anchor": ["core/security.py", ".github/workflows/security_audit.yaml"],
      "acceptance_signal": "Run audit; verify no hardcoded secrets in code; all API keys loaded from Secret Manager",
      "urgency": "P0",
      "owner_hint": "Security"
    },
    {
      "id": "Q_116",
      "area": "CI/CD",
      "question": "How do we enforce green tests before deploy and block deployment on test failures?",
      "why_it_matters": "Q_092 requires deployment safety; failed tests may not block deploy; no gate visible in CI/CD",
      "evidence_anchor": [".github/workflows/deploy.yaml", ".github/workflows/test.yaml"],
      "acceptance_signal": "Fail unit test; verify deploy job blocked with test_failure gate",
      "urgency": "P0",
      "owner_hint": "DevOps"
    },
    {
      "id": "Q_117",
      "area": "Features",
      "question": "How do we measure and alert on feature drift between online and offline Feature Store?",
      "why_it_matters": "Q_057 and A_008 require parity; train/serve skew causes model degradation; no drift detector visible",
      "evidence_anchor": ["core/feature_store.py", "agents/data_quality.py"],
      "acceptance_signal": "Inject 1% drift; verify alert fires with feature_drift_detected and diff metrics",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_118",
      "area": "Strategy",
      "question": "How do we validate MMM holiday controls and seasonality effects in model?",
      "why_it_matters": "A_009 requires contract drift prevention; holiday effects may be missing; impacts budget allocation",
      "evidence_anchor": ["agents/mmm_agent.py", "tests/test_mmm_agent.py"],
      "acceptance_signal": "Run MMM with holiday data; verify coefficients for holidays and seasonality in output",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_119",
      "area": "Observability",
      "question": "How do we detect and alert on schema drift in ingested data streams?",
      "why_it_matters": "A_010 requires schema validation; contract drift causes pipeline failures; no drift detector visible",
      "evidence_anchor": ["agents/schema_validator.py", "agents/data_quality.py"],
      "acceptance_signal": "Ingest data with extra field; verify alert fires with schema_drift_detected and field diff",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_120",
      "area": "Risk",
      "question": "How do we enforce 24h timeout for approval requests with auto-escalation?",
      "why_it_matters": "A_011 requires playbook risk gate; stale approvals block execution; no timeout visible in approval logic",
      "evidence_anchor": ["agents/approval_agent.py", "agents/playbook_orchestrator.py"],
      "acceptance_signal": "Submit approval; wait 24h; verify auto-escalation to manager with timeout_escalation event",
      "urgency": "P1",
      "owner_hint": "Governance"
    },
    {
      "id": "Q_121",
      "area": "Observability",
      "question": "How do we enforce freshness SLA=6h for ingested data and alert on staleness?",
      "why_it_matters": "A_012 requires data quality; stale data causes incorrect decisions; no freshness check visible",
      "evidence_anchor": ["agents/data_quality.py", "core/message_bus.py"],
      "acceptance_signal": "Simulate 7h staleness; verify alert fires with data_freshness_violation and timestamp diff",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_122",
      "area": "Compliance",
      "question": "How do we enforce immutable audit logs with 7yr retention policy?",
      "why_it_matters": "A_015 requires compliance; GDPR/SOX mandate audit trail; no immutability guarantee visible",
      "evidence_anchor": ["core/audit.py", "infrastructure/database.tf"],
      "acceptance_signal": "Attempt to modify audit log; verify rejection; query 7yr old logs successfully",
      "urgency": "P1",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_123",
      "area": "Data",
      "question": "How do we handle multi-currency revenue with historical FX rates for accurate MMM?",
      "why_it_matters": "A_016 requires data integrity; currency mixing causes incorrect attribution; no FX conversion visible",
      "evidence_anchor": ["core/contracts.py", "agents/mmm_agent.py"],
      "acceptance_signal": "Ingest revenue in JPY and USD; verify MMM uses historical FX for conversion with rate_date logged",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_124",
      "area": "Risk",
      "question": "How do we enforce rate limiting per endpoint and user with metrics emission?",
      "why_it_matters": "A_017 requires infra protection; API abuse prevention; no rate limiter visible in API gateway",
      "evidence_anchor": ["core/rate_limiter.py", "infrastructure/api_gateway.py"],
      "acceptance_signal": "Exceed rate limit; verify HTTP 429 with rate_limit_exceeded and prometheus counter incremented",
      "urgency": "P1",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_125",
      "area": "Data",
      "question": "How do we detect duplicate creative assets via fingerprinting before upload?",
      "why_it_matters": "A_018 requires asset management; duplicates waste storage and confuse analytics; no fingerprint visible",
      "evidence_anchor": ["core/asset_fingerprint.py", "agents/creative_intelligence.py"],
      "acceptance_signal": "Upload duplicate asset; verify rejection with duplicate_asset_detected and existing asset_id",
      "urgency": "P2",
      "owner_hint": "Creative"
    },
    {
      "id": "Q_126",
      "area": "Latency",
      "question": "What is the p99 latency SLA for MTA attribution computation?",
      "why_it_matters": "Real-time decisioning requires low latency; no SLA documented; impacts playbook execution speed",
      "evidence_anchor": ["agents/mta_agent.py", "SSOT/METRICS/sla.yaml"],
      "acceptance_signal": "Query p99 latency metric; verify <500ms documented in SLA; alert fires if exceeded",
      "urgency": "P1",
      "owner_hint": "Performance"
    },
    {
      "id": "Q_127",
      "area": "Execution",
      "question": "How do we ensure bid mutations are idempotent when Kafka retries occur?",
      "why_it_matters": "Similar to Q_056 activation dedup; bid changes need operation_id tracking; no dedup in bid agent",
      "evidence_anchor": ["agents/bid_guidance_agent.py", "agents/activation_agent.py"],
      "acceptance_signal": "Send bid update twice; verify single mutation with operation_id dedup logged",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_128",
      "area": "Backtest",
      "question": "How do we prevent look-ahead bias in backtests by enforcing point-in-time data access?",
      "why_it_matters": "Look-ahead bias inflates backtest results; Feature Store must serve historical snapshots only",
      "evidence_anchor": ["core/feature_store.py", "tests/test_backtest.py"],
      "acceptance_signal": "Request future feature; verify rejection with point_in_time_violation error",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_129",
      "area": "SimLab",
      "question": "How do we validate simulator accuracy by comparing to production metrics?",
      "why_it_matters": "Sim/prod divergence invalidates testing; no accuracy benchmark visible in SimLab",
      "evidence_anchor": ["simlab/simulator.py", "tests/test_simulator.py"],
      "acceptance_signal": "Run simulator with prod data; verify <5% error on ROAS, CPA metrics vs production",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_130",
      "area": "RAG",
      "question": "How do we enforce vector embedding version compatibility when retriever is upgraded?",
      "why_it_matters": "Embedding model changes break retrieval; no versioning visible in RAG pipeline",
      "evidence_anchor": ["agents/llm_council.py", "core/vector_db.py"],
      "acceptance_signal": "Upgrade embeddings model; verify migration script re-indexes with version tag",
      "urgency": "P1",
      "owner_hint": "LLM"
    },
    {
      "id": "Q_131",
      "area": "Governance",
      "question": "How do we enforce RBAC for Feature Store access by team/role?",
      "why_it_matters": "Sensitive features require access control; no RBAC visible in Feature Store API",
      "evidence_anchor": ["core/feature_store.py", "core/auth.py"],
      "acceptance_signal": "Request restricted feature without permission; verify HTTP 403 with rbac_denied error",
      "urgency": "P1",
      "owner_hint": "Security"
    },
    {
      "id": "Q_132",
      "area": "Infra",
      "question": "How do we handle graceful shutdown for in-flight agent tasks on pod termination?",
      "why_it_matters": "Kubernetes evictions can kill tasks mid-execution; no SIGTERM handler visible",
      "evidence_anchor": ["agents/base_agent.py", "infrastructure/k8s/deployment.yaml"],
      "acceptance_signal": "Send SIGTERM; verify in-flight task completes or checkpoints before exit with graceful_shutdown logged",
      "urgency": "P1",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_133",
      "area": "Observability",
      "question": "How do we emit custom metrics for playbook execution success/failure rates?",
      "why_it_matters": "Playbook reliability requires metrics; no playbook_execution_total counter visible",
      "evidence_anchor": ["agents/playbook_orchestrator.py", "core/metrics.py"],
      "acceptance_signal": "Run playbook; verify prometheus counter playbook_execution_total{status=success|failure} incremented",
      "urgency": "P1",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_134",
      "area": "CI/CD",
      "question": "How do we enforce Docker image scanning for CVEs before deployment?",
      "why_it_matters": "Vulnerable images risk production; no scanning step visible in CI/CD",
      "evidence_anchor": [".github/workflows/build.yaml", "infrastructure/k8s/image_policy.yaml"],
      "acceptance_signal": "Build image with CVE; verify scan blocks deployment with vulnerability_detected error",
      "urgency": "P1",
      "owner_hint": "Security"
    },
    {
      "id": "Q_135",
      "area": "Data",
      "question": "How do we handle late-arriving events in Event Bus with out-of-order timestamps?",
      "why_it_matters": "Late events skew aggregations; no timestamp watermark visible in event processing",
      "evidence_anchor": ["core/message_bus.py", "agents/feature_engineering.py"],
      "acceptance_signal": "Send late event; verify windowed aggregation includes it with late_event_processed logged",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_136",
      "area": "Strategy",
      "question": "How do we calibrate MMM with incrementality test results to adjust attribution?",
      "why_it_matters": "MMM may overestimate incrementality; no calibration visible in MMM training",
      "evidence_anchor": ["agents/mmm_agent.py", "agents/incrementality_agent.py"],
      "acceptance_signal": "Run incrementality test; verify MMM coefficients adjusted with calibration_applied logged",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_137",
      "area": "Planner",
      "question": "How do we enforce minimum test duration for A/B tests before declaring a winner?",
      "why_it_matters": "Early stopping inflates false positives; no minimum duration gate visible",
      "evidence_anchor": ["agents/ab_test_agent.py", "config/test_policies.yaml"],
      "acceptance_signal": "Stop test after 1 day; verify rejection with minimum_duration_violation error",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_138",
      "area": "Critic",
      "question": "How do we detect and alert on anomalous ROAS drops across campaigns?",
      "why_it_matters": "Sudden drops indicate issues; no anomaly detector visible in monitoring",
      "evidence_anchor": ["agents/monitoring_agent.py", "core/metrics.py"],
      "acceptance_signal": "Inject 50% ROAS drop; verify alert fires with anomaly_detected and campaign_id",
      "urgency": "P1",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_139",
      "area": "Rules",
      "question": "How do we version and rollback playbook definitions when a playbook fails?",
      "why_it_matters": "Bad playbooks can cascade failures; no versioning visible in playbook storage",
      "evidence_anchor": ["agents/playbook_orchestrator.py", "SSOT/PLAYBOOKS/"],
      "acceptance_signal": "Deploy bad playbook; verify rollback to previous version with playbook_rollback logged",
      "urgency": "P1",
      "owner_hint": "Governance"
    },
    {
      "id": "Q_140",
      "area": "Risk",
      "question": "How do we enforce kill-switch activation latency <5s for emergency stops?",
      "why_it_matters": "Slow kill-switches fail during crises; no latency SLA visible",
      "evidence_anchor": ["core/circuit_breaker.py", "agents/activation_agent.py"],
      "acceptance_signal": "Trigger kill-switch; verify all executions stop in <5s with kill_switch_activated logged",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_141",
      "area": "Execution",
      "question": "How do we handle ad platform API rate limits with exponential backoff?",
      "why_it_matters": "Rate limit violations cause execution failures; no backoff visible in API clients",
      "evidence_anchor": ["agents/activation_agent.py", "core/http_client.py"],
      "acceptance_signal": "Hit rate limit; verify exponential backoff with retry_after_seconds logged",
      "urgency": "P1",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_142",
      "area": "Data",
      "question": "How do we validate data lineage for compliance audits with end-to-end tracking?",
      "why_it_matters": "Regulatory compliance requires lineage; no lineage tracking visible in data pipeline",
      "evidence_anchor": ["core/data_lineage.py", "agents/data_quality.py"],
      "acceptance_signal": "Query lineage for revenue metric; verify full upstream dag with source dataset IDs",
      "urgency": "P1",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_143",
      "area": "Features",
      "question": "How do we enforce feature freshness thresholds and reject stale features in online serving?",
      "why_it_matters": "Stale features degrade model accuracy; no freshness check in online Feature Store",
      "evidence_anchor": ["core/feature_store.py", "agents/feature_engineering.py"],
      "acceptance_signal": "Request feature >6h old; verify rejection with stale_feature error and timestamp",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_144",
      "area": "Strategy",
      "question": "How do we detect budget pacing anomalies and alert when overspend >20% of daily target?",
      "why_it_matters": "Pacing failures waste budget; no overspend detector visible in pacing agent",
      "evidence_anchor": ["agents/pacing_agent.py", "agents/monitoring_agent.py"],
      "acceptance_signal": "Spend 30% over target; verify alert fires with budget_overpacing_detected and pct_over",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_145",
      "area": "Compliance",
      "question": "How do we enforce promo/ad labeling in all non-Japanese languages per local regulations?",
      "why_it_matters": "Q_053 covers JP/VN; other markets have similar requirements; no multi-locale check visible",
      "evidence_anchor": ["agents/compliance_agent.py", "config/compliance_rules.yaml"],
      "acceptance_signal": "Submit creative in Spanish without label; verify rejection with promo_label_required error",
      "urgency": "P1",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_146",
      "area": "RAG",
      "question": "How do we cache LLM responses with prompt_hash to reduce API costs?",
      "why_it_matters": "Repeated queries waste LLM budget; no caching visible in LLM client",
      "evidence_anchor": ["agents/llm_council.py", "core/cache.py"],
      "acceptance_signal": "Call LLM twice with same prompt; verify second call returns cached with cache_hit logged",
      "urgency": "P2",
      "owner_hint": "LLM"
    },
    {
      "id": "Q_147",
      "area": "Observability",
      "question": "How do we aggregate distributed traces across services to compute end-to-end latency?",
      "why_it_matters": "Multi-service traces require aggregation; no trace aggregation visible in observability",
      "evidence_anchor": ["core/tracing.py", "infrastructure/jaeger.yaml"],
      "acceptance_signal": "Query trace by correlation_id; verify spans from all services aggregated with total_duration",
      "urgency": "P1",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_148",
      "area": "RBAC",
      "question": "How do we enforce row-level security for Feature Store based on data classification?",
      "why_it_matters": "PII features require restricted access; no RLS visible in Feature Store queries",
      "evidence_anchor": ["core/feature_store.py", "core/auth.py"],
      "acceptance_signal": "Query PII feature without clearance; verify HTTP 403 with row_level_security_denied error",
      "urgency": "P1",
      "owner_hint": "Security"
    },
    {
      "id": "Q_149",
      "area": "Infra",
      "question": "How do we enforce resource quotas per team/namespace to prevent noisy neighbor issues?",
      "why_it_matters": "Resource contention degrades performance; no quotas visible in Kubernetes config",
      "evidence_anchor": ["infrastructure/k8s/resource_quota.yaml", "infrastructure/k8s/namespace.yaml"],
      "acceptance_signal": "Exceed CPU quota; verify pod scheduling blocked with quota_exceeded error",
      "urgency": "P1",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_150",
      "area": "CI/CD",
      "question": "How do we enforce semantic versioning and changelog generation for releases?",
      "why_it_matters": "Version tracking prevents rollback confusion; no version enforcement visible in CI/CD",
      "evidence_anchor": [".github/workflows/release.yaml", "CHANGELOG.md"],
      "acceptance_signal": "Create release without semver tag; verify rejection with invalid_version error",
      "urgency": "P2",
      "owner_hint": "DevOps"
    }
  ]
}
