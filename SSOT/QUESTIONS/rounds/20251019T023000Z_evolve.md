{
  "questions": [
    {
      "id": "Q_030",
      "area": "Risk",
      "question": "How are kill-switches tested to ensure <5s response time in production?",
      "why_it_matters": "Q_016 requires kill-switch <5s; C17_ActivationAgent CRITICAL; no test evidence in coverage; runtime safety depends on instant halt capability",
      "evidence_anchor": ["tests/agents/test_activation_kill_switch.py", "agents/activation_agent.py", "SSOT/COVERAGE/coverage.matrix.csv"],
      "acceptance_signal": "Test exists verifying kill-switch halt within 5s; Prometheus metric kill_switch_halt_latency_seconds <5.0; CI passes",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_031",
      "area": "Execution",
      "question": "What prevents duplicate bid mutations when activation agent retries after timeout?",
      "why_it_matters": "Q_127 idempotency enforcement; C17 CRITICAL; auction market damage from duplicate bids; no retry logic tests found",
      "evidence_anchor": ["tests/agents/test_activation_idempotency.py", "agents/activation_agent.py", "core/contracts.py"],
      "acceptance_signal": "Test verifies request_id dedup on retry; metric duplicate_bid_mutations_prevented increments; zero double-spend in last 30d",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_032",
      "area": "Data",
      "question": "How is multi-currency revenue normalized to prevent MMM bias?",
      "why_it_matters": "A_016 MAJOR+P1 gap; DataOps_Revenue Red; JPY/USD/EUR mixing causes ±20% MMM coefficient errors; no FX normalization found",
      "evidence_anchor": ["tests/core/test_revenue_currency_normalization.py", "core/contracts.py", "agents/ecommerce_agent.py"],
      "acceptance_signal": "Test passes multi-currency order normalization to base_currency; historical_fx_rate lookup working; MMM MAPE <15%",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_033",
      "area": "Compliance",
      "question": "Are audit logs cryptographically immutable with tamper detection?",
      "why_it_matters": "A_015 MAJOR+P1; Compliance_AuditLog Red; 7yr retention requires tamper-proof; no hash chain or signing found",
      "evidence_anchor": ["tests/core/test_audit_log_immutability.py", "core/audit.py", "SSOT/COVERAGE/coverage.matrix.csv"],
      "acceptance_signal": "Test verifies hash chain breaks on tamper; append-only storage enforced; audit log tampering attempt logged",
      "urgency": "P1",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_034",
      "area": "Governance",
      "question": "What is the maximum approval timeout before playbook auto-escalation triggers?",
      "why_it_matters": "A_011 MINOR+P1; C16_PlaybookOrchestrator Red; budget allocation stalls without escalation; no timeout config found",
      "evidence_anchor": ["tests/agents/test_playbook_approval_timeout.py", "agents/approval_agent.py", "playbooks/budget_reallocation.yaml"],
      "acceptance_signal": "Test verifies 24h timeout escalates to CMO; metric approval_escalations_total increments; no >24h stuck approvals",
      "urgency": "P1",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_035",
      "area": "Observability",
      "question": "Is distributed tracing instrumented end-to-end from gateway to database?",
      "why_it_matters": "Q_078/Q_018 CRITICAL; Infra_Observability Red; MTTR depends on E2E trace; no OTEL spans found in code",
      "evidence_anchor": ["tests/infra/test_otel_tracing.py", "infrastructure/otel.py", "agents/*.py"],
      "acceptance_signal": "Test verifies trace_id propagates API→Agent→DB; latency breakdown <5s visible in Jaeger; P95 latency SLO met",
      "urgency": "P0",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_036",
      "area": "RBAC",
      "question": "How is JWT role enforcement tested to prevent privilege escalation?",
      "why_it_matters": "Q_075/Q_019 CRITICAL; Infra_Auth Red; ad-ops→admin escalation risk; no JWT RBAC tests found",
      "evidence_anchor": ["tests/infra/test_jwt_rbac.py", "middleware/auth.py", "core/rbac.py"],
      "acceptance_signal": "Test verifies ad-ops role gets 403 on admin endpoint; JWT tampering detected; metric unauthorized_access_attempts increments",
      "urgency": "P0",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_037",
      "area": "Secrets",
      "question": "Are all secrets sourced from Secret Manager with zero env var usage?",
      "why_it_matters": "Q_021/Q_090 CRITICAL; DataOps_Secrets Red; env var leaks to logs; no secret manager integration found",
      "evidence_anchor": ["tests/infra/test_secret_manager.py", "core/security.py", "CI audit script"],
      "acceptance_signal": "CI audit fails if any .env SECRET_ var found; runtime fetches from Secret Manager only; zero hardcoded secrets in 7d scan",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_038",
      "area": "CI/CD",
      "question": "Does CI block deploy if any pytest fails?",
      "why_it_matters": "Q_092/Q_023 CRITICAL done; verify enforcement persists; CI_CD Red; regression risk without strict gate",
      "evidence_anchor": [".github/workflows/deploy.yaml", "tests/ci/test_pytest_ci_gate.py"],
      "acceptance_signal": "Workflow fails on pytest exit!=0; deploy never proceeds if tests fail; metric failed_deploys_blocked increments",
      "urgency": "P0",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_039",
      "area": "Observability",
      "question": "What is the P0 alert delivery SLA to PagerDuty?",
      "why_it_matters": "Q_100/Q_024 CRITICAL done; verify <2min SLA maintained; Monitoring_Alerting Red; MTTR depends on fast escalation",
      "evidence_anchor": ["infrastructure/alertmanager.yaml", "tests/agents/test_alert_agent_pagerduty.py", "SSOT/METRICS/progress.json"],
      "acceptance_signal": "Alert fires within 120s of P0 trigger; PagerDuty API receives event <2min; metric alert_delivery_latency_seconds P99<120",
      "urgency": "P0",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_040",
      "area": "Data",
      "question": "How is schema backward compatibility enforced across v1/v2 migrations?",
      "why_it_matters": "Q_025/Q_071 done; verify enforcement; DataOps_Validation Red; v2 breaking v1 clients causes data loss",
      "evidence_anchor": ["tests/core/test_schema_backward_compat.py", "core/contracts.py"],
      "acceptance_signal": "Test fails on breaking change; v2 send→v1 receive success; CI blocks incompatible schema merge",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_041",
      "area": "Data",
      "question": "Are timezone mismatches in Shopify orders normalized to UTC with metadata?",
      "why_it_matters": "Q_028/A_016 done; verify implementation; ±15% MMM skew from TZ bugs; DataOps_Revenue critical",
      "evidence_anchor": ["tests/agents/test_ecommerce_timezone.py", "agents/ecommerce_agent.py", "core/contracts.py"],
      "acceptance_signal": "Test verifies normalized_utc + original_tz stored; MMM input in UTC only; zero TZ mismatch errors in 7d",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_042",
      "area": "Data",
      "question": "Does Meta API v19 schema change detection trigger CI failure?",
      "why_it_matters": "Q_029/A_010 done; verify backward compat CI gate active; v18→v19 field removal broke production",
      "evidence_anchor": ["tests/agents/test_ad_platform_meta_v19.py", "agents/schema_validator.py", ".github/workflows/ci.yaml"],
      "acceptance_signal": "CI fails on breaking schema change; migration guide generated; alert fires <5min; metric schema_drift_detected increments",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_043",
      "area": "Features",
      "question": "How is online/offline feature parity verified in Feature Store?",
      "why_it_matters": "Q_057/A_008 CRITICAL; C03_FeatureEngineering Red; training/serving skew causes ±10% model drift; no parity tests",
      "evidence_anchor": ["tests/core/test_feature_store_parity.py", "core/feature_store.py", "agents/feature_engineering_agent.py"],
      "acceptance_signal": "Test verifies same feature values batch vs. streaming; parity check passes daily; metric feature_skew_percent <1%",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_044",
      "area": "Strategy",
      "question": "What feature drift detector alerts when model inputs shift >10%?",
      "why_it_matters": "Q_117 HIGH; C03 Red; silent drift degrades MMM/MTA; no drift monitoring found",
      "evidence_anchor": ["tests/agents/test_feature_drift_detector.py", "agents/monitoring_agent.py", "core/feature_store.py"],
      "acceptance_signal": "Test verifies alert fires on >10% distribution shift; metric feature_drift_score computed; dashboard shows P95 values",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_045",
      "area": "Strategy",
      "question": "Are MMM seasonality controls tested with holiday dummy variables?",
      "why_it_matters": "A_009 HIGH; C04_MMM Red; Christmas/Black Friday cause ±30% spikes; no seasonality controls found",
      "evidence_anchor": ["tests/agents/test_mmm_seasonality.py", "agents/mmm_agent.py", "data/holiday_calendar.csv"],
      "acceptance_signal": "Test verifies holiday dummies included; MMM MAPE <15% on seasonal data; Bayesian trace shows seasonality coefficients",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_046",
      "area": "Risk",
      "question": "How is MTA path aggregation enforced to require >=10 users per path?",
      "why_it_matters": "Q_007 CRITICAL; C05_MTA Red; <10 users violates privacy; no aggregation enforcement found",
      "evidence_anchor": ["tests/agents/test_mta_privacy.py", "agents/mta_agent.py", "core/contracts.py"],
      "acceptance_signal": "Test verifies paths <10 users dropped; metric privacy_violations_total=0; MTA output only aggregated paths",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_047",
      "area": "RAG",
      "question": "Is LLM Analyst model strictly different from Verifier model?",
      "why_it_matters": "Q_010/Q_074 CRITICAL; C10_LLMCouncil Red; same model analyst+verifier defeats verification; no config enforcement",
      "evidence_anchor": ["tests/agents/test_llm_council_separation.py", "agents/llm_council.py", "config/models.yaml"],
      "acceptance_signal": "Test fails if analyst_model==verifier_model; config enforces claude-sonnet≠claude-opus; metric model_separation_enforced=1",
      "urgency": "P0",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_048",
      "area": "RAG",
      "question": "Are all LLM outputs validated to have >=2 source_ids before acceptance?",
      "why_it_matters": "Q_073 CRITICAL; C10 Red; single-source hallucination risk; no source count validation found",
      "evidence_anchor": ["tests/agents/test_llm_source_validation.py", "agents/llm_council.py", "core/guardrails.py"],
      "acceptance_signal": "Test verifies output rejected if source_ids.length<2; metric llm_insufficient_sources_total increments; zero <2 source outputs",
      "urgency": "P0",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_049",
      "area": "RAG",
      "question": "What enforces LLM temperature <=0.2 for deterministic outputs?",
      "why_it_matters": "Q_072 CRITICAL; C10 Red; temperature >0.2 causes non-reproducible decisions; no config validation found",
      "evidence_anchor": ["tests/agents/test_llm_temperature.py", "agents/llm_council.py", "config/models.yaml"],
      "acceptance_signal": "Test fails if temperature>0.2 in any LLM call; config schema enforces max 0.2; metric llm_temperature_violations=0",
      "urgency": "P0",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_050",
      "area": "Risk",
      "question": "How does Crisis Detection cap risk_score at 0.5 when <2 independent sources exist?",
      "why_it_matters": "Q_011/Q_088 CRITICAL; C11_CrisisDetection Red; single-source crisis triggers false pauses; no source count validation",
      "evidence_anchor": ["tests/agents/test_crisis_source_validation.py", "agents/crisis_detection_agent.py", "core/guardrails.py"],
      "acceptance_signal": "Test verifies risk_score capped at 0.5 if sources<2; metric crisis_low_confidence_total increments; zero high-risk single-source alerts",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_051",
      "area": "Compliance",
      "question": "Is age <18 enforcement tested to return 403 on restricted endpoints?",
      "why_it_matters": "Q_012/Q_097 CRITICAL; C12_ComplianceAgent Red; minor access to promo violates law; no age gate tests found",
      "evidence_anchor": ["tests/agents/test_compliance_age_gate.py", "agents/compliance_agent.py", "middleware/age_verification.py"],
      "acceptance_signal": "Test verifies age<18 gets 403; metric age_gate_blocks_total increments; zero minor access in 30d audit",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_052",
      "area": "Compliance",
      "question": "Are all Japan promotional messages enforced to include Promo/広告 label?",
      "why_it_matters": "Q_013/Q_097 CRITICAL; C12 Red; Japan law requires promo disclosure; no label enforcement found",
      "evidence_anchor": ["tests/agents/test_compliance_promo_label_japan.py", "agents/compliance_agent.py", "core/guardrails.py"],
      "acceptance_signal": "Test fails if Japan promo lacks Promo/広告; metric promo_label_violations_total=0; CI blocks unlabeled Japan ads",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_053",
      "area": "Execution",
      "question": "What budget allocation limits prevent >25% single-channel shift per week?",
      "why_it_matters": "Q_082/Q_103 CRITICAL; C13_BudgetAllocation Red; 50% shift broke campaign; no shift limit tests found",
      "evidence_anchor": ["tests/agents/test_budget_shift_limits.py", "agents/budget_allocation_agent.py", "playbooks/budget_reallocation.yaml"],
      "acceptance_signal": "Test verifies allocation rejected if shift>25%; metric budget_shift_violations_total increments; approval required for >25%",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_054",
      "area": "Execution",
      "question": "How is budget allocation idempotency enforced to prevent double-apply?",
      "why_it_matters": "Q_082 CRITICAL; C13 Red; retry after timeout causes 2x spend; no allocation_id dedup found",
      "evidence_anchor": ["tests/agents/test_budget_idempotency.py", "agents/budget_allocation_agent.py", "core/contracts.py"],
      "acceptance_signal": "Test verifies allocation_id dedup on retry; metric duplicate_allocations_prevented increments; zero double-spend in 30d",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_055",
      "area": "Infra",
      "question": "What circuit breaker halts external API calls after 3 consecutive failures?",
      "why_it_matters": "Q_140/A_017 CRITICAL; Infra_ExternalAPIs Red; cascading Meta API failures; no circuit breaker found",
      "evidence_anchor": ["tests/infra/test_circuit_breaker.py", "infrastructure/circuit_breaker.py", "agents/ad_platform_agent.py"],
      "acceptance_signal": "Test verifies open circuit after 3 fails; metric circuit_breaker_opened_total increments; API calls halted <5s",
      "urgency": "P0",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_056",
      "area": "Infra",
      "question": "How is rate limiting enforced to return 429 on 101st request in burst window?",
      "why_it_matters": "Q_017/A_017 CRITICAL; Infra_ExternalAPIs Red; 100 req/min burst violated; no rate limiter tests found",
      "evidence_anchor": ["tests/infra/test_rate_limiter.py", "infrastructure/rate_limiter.py", "middleware/throttle.py"],
      "acceptance_signal": "Test verifies 101st req gets 429; metric rate_limit_exceeded_total increments; P99 latency unaffected by throttle",
      "urgency": "P0",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_057",
      "area": "Infra",
      "question": "What prevents BigQuery rate limit exhaustion at 10k queries/sec?",
      "why_it_matters": "Q_026 relates to Q_017; MMM batch jobs hit quota; no BQ rate limiter found",
      "evidence_anchor": ["tests/infra/test_bigquery_rate_limit.py", "infrastructure/rate_limiter.py", "agents/mmm_agent.py"],
      "acceptance_signal": "Test verifies BQ queries throttled <10k/sec; metric bigquery_throttled_queries increments; zero quota exhaustion in 7d",
      "urgency": "P1",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_058",
      "area": "RBAC",
      "question": "Is PII mapping table restricted to pii_admin role only?",
      "why_it_matters": "Q_020/Q_075 CRITICAL; Infra_Auth Red; email_hash→email exposure risk; no RBAC enforcement found",
      "evidence_anchor": ["tests/infra/test_rbac_pii_mapping.py", "core/rbac.py", "models/pii_mapping.sql"],
      "acceptance_signal": "Test verifies non-pii_admin gets 403 on pii_mapping query; metric unauthorized_pii_access_attempts increments; zero leaks",
      "urgency": "P0",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_059",
      "area": "Execution",
      "question": "How is HMAC-SHA256 webhook signature validation enforced to prevent spoofing?",
      "why_it_matters": "Q_022/Q_098 done; verify enforcement; Execution; spoofed Shopify webhook causes data corruption",
      "evidence_anchor": ["tests/agents/test_webhook_hmac_validation.py", "agents/webhook_receiver.py", "middleware/hmac_validator.py"],
      "acceptance_signal": "Test verifies invalid HMAC gets 401; metric webhook_validation_failures_total increments; zero spoofed webhook accepted",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_060",
      "area": "Data",
      "question": "What enforces MIN_RECORDS=100 rejection for batch feature engineering?",
      "why_it_matters": "Q_005/Q_101 HIGH; C03 Red; <100 records causes high-variance features; no min record check found",
      "evidence_anchor": ["tests/agents/test_feature_min_records.py", "agents/feature_engineering_agent.py", "core/contracts.py"],
      "acceptance_signal": "Test verifies batch rejected if records<100; metric low_record_batches_rejected increments; zero <100 record batches processed",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_061",
      "area": "Strategy",
      "question": "How is MMM model validation tested against holdout set with MAPE <15%?",
      "why_it_matters": "C04_MMM Red; overfitting risk; no validation tests found; production models unvalidated",
      "evidence_anchor": ["tests/agents/test_mmm_validation.py", "agents/mmm_agent.py", "models/mmm_bayesian.py"],
      "acceptance_signal": "Test verifies holdout MAPE <15%; metric mmm_validation_mape computed; CI blocks if MAPE>15%",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_062",
      "area": "Observability",
      "question": "Are all critical component failures logged with trace_id for distributed debugging?",
      "why_it_matters": "Infra_Observability Red; MTTR >10min without trace context; no trace_id propagation found",
      "evidence_anchor": ["tests/infra/test_trace_logging.py", "infrastructure/logging.py", "middleware/tracing.py"],
      "acceptance_signal": "Test verifies error logs contain trace_id; Jaeger shows end-to-end trace; P95 error resolution time <5min",
      "urgency": "P1",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_063",
      "area": "Data",
      "question": "How are event_id duplicates detected and rejected in ingestion pipeline?",
      "why_it_matters": "A_005 CRITICAL; C01_IdentityResolution Red; duplicate events cause 2x revenue; no dedup found",
      "evidence_anchor": ["tests/agents/test_event_dedup.py", "agents/analytics_agent.py", "core/contracts.py"],
      "acceptance_signal": "Test verifies duplicate event_id rejected; metric duplicate_events_rejected increments; zero duplicate events in 7d",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_064",
      "area": "Secrets",
      "question": "Is PII salt rotated every 90d with Secret Manager dual-valid window?",
      "why_it_matters": "Q_002/Q_070 done; verify 90d rotation active; GDPR requires regular rotation; no rotation schedule found",
      "evidence_anchor": ["tests/core/test_security_salt_rotation_90d.py", "core/security.py", "infrastructure/secret_manager.py"],
      "acceptance_signal": "Test verifies salt rotates every 90d; metric days_since_last_salt_rotation <90; dual-valid window 24h",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_065",
      "area": "Data",
      "question": "What prevents GA4 export_time >6h from corrupting MTA paths?",
      "why_it_matters": "Q_004/Q_027/A_012 done; verify rejection active; stale GA4 causes ±15% MTA error",
      "evidence_anchor": ["tests/agents/test_data_quality_freshness_ga4.py", "agents/data_quality.py", "agents/analytics_agent.py"],
      "acceptance_signal": "Test verifies GA4 export rejected if >6h old; metric data_freshness_violations_ga4 increments; zero stale GA4 processed",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_066",
      "area": "Features",
      "question": "How is Feature Store tested for online/offline feature value equality?",
      "why_it_matters": "Q_057/A_008 CRITICAL; C03 Red; training!=serving causes model drift; no parity CI test",
      "evidence_anchor": ["tests/core/test_feature_store_online_offline_parity.py", "core/feature_store.py"],
      "acceptance_signal": "CI test compares batch vs streaming features; parity check passes; metric feature_parity_violations=0",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_067",
      "area": "Strategy",
      "question": "What Creative Intelligence asset fingerprint prevents duplicate upload?",
      "why_it_matters": "A_018 MEDIUM; C08_CreativeIntelligence Red; duplicate assets waste spend; no fingerprint found",
      "evidence_anchor": ["tests/agents/test_creative_asset_fingerprint.py", "agents/creative_intelligence_agent.py", "core/contracts.py"],
      "acceptance_signal": "Test verifies perceptual hash dedup; metric duplicate_assets_rejected increments; zero duplicate uploads in 30d",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_068",
      "area": "Governance",
      "question": "How is playbook dry-run mode tested to prevent production mutations?",
      "why_it_matters": "C16_PlaybookOrchestrator Red; dry-run critical for safe testing; no dry-run enforcement found",
      "evidence_anchor": ["tests/agents/test_playbook_dry_run.py", "agents/playbook_orchestrator.py", "playbooks/*.yaml"],
      "acceptance_signal": "Test verifies dry-run=true prevents API writes; metric dry_run_executions_total increments; zero mutations in dry-run",
      "urgency": "P1",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_069",
      "area": "Observability",
      "question": "Are Prometheus metrics exported for all CRITICAL components with <1s scrape interval?",
      "why_it_matters": "C19_MonitoringAgent Red; no metrics found for C01/C05/C10/C11/C12/C13/C17; blind spots in critical path",
      "evidence_anchor": ["tests/infra/test_prometheus_metrics.py", "infrastructure/prometheus.yaml", "agents/*.py"],
      "acceptance_signal": "Test verifies all CRITICAL components export metrics; scrape interval <1s; Grafana dashboards populated",
      "urgency": "P1",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_070",
      "area": "Data",
      "question": "What prevents identity resolution hash collisions with salt rotation?",
      "why_it_matters": "Q_002 done; verify collision detection; SHA256 collision causes user merge; no collision monitoring",
      "evidence_anchor": ["tests/core/test_identity_hash_collision.py", "core/security.py", "agents/identity_resolution_agent.py"],
      "acceptance_signal": "Test verifies collision detection; metric identity_hash_collisions_total=0; alert fires on collision",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_071",
      "area": "Infra",
      "question": "How is Secret Manager fetch timeout enforced at 5s with cached fallback?",
      "why_it_matters": "Q_002 related; Secret Manager outage blocks app start; no timeout/fallback found",
      "evidence_anchor": ["tests/infra/test_secret_manager_timeout.py", "infrastructure/secret_manager.py", "core/security.py"],
      "acceptance_signal": "Test verifies 5s timeout; fallback to cached secret; metric secret_fetch_timeouts increments; app remains available",
      "urgency": "P1",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_072",
      "area": "Compliance",
      "question": "Is audit log retention tested to enforce 7yr immutable storage?",
      "why_it_matters": "A_015 MAJOR+P1; Compliance_AuditLog Red; GDPR/SOX requires 7yr; no retention enforcement found",
      "evidence_anchor": ["tests/core/test_audit_log_retention_7yr.py", "core/audit.py", "infrastructure/storage.py"],
      "acceptance_signal": "Test verifies 7yr retention policy; logs older than 7yr archived; metric audit_log_age_max_days monitored",
      "urgency": "P1",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_073",
      "area": "Risk",
      "question": "What prevents playbook execution without explicit approval for high-risk changes?",
      "why_it_matters": "A_011/C16 P1; auto-execution of >$50k budget shift needs approval; no approval gate found",
      "evidence_anchor": ["tests/agents/test_playbook_approval_gate.py", "agents/playbook_orchestrator.py", "agents/approval_agent.py"],
      "acceptance_signal": "Test verifies high-risk playbook blocked until approved; metric approval_required_executions increments; zero auto-exec >$50k",
      "urgency": "P1",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_074",
      "area": "Data",
      "question": "How is historical FX rate lookup tested for multi-currency revenue normalization?",
      "why_it_matters": "Q_032/A_016 P1; MMM requires FX at order_date; no historical FX found; spot rate causes bias",
      "evidence_anchor": ["tests/core/test_fx_rate_historical.py", "core/contracts.py", "data/fx_rates.csv"],
      "acceptance_signal": "Test verifies FX rate at order_date used; metric fx_rate_mismatches=0; MMM input currency normalized to USD",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_075",
      "area": "Observability",
      "question": "Are all agent errors logged with structured JSON for parsing?",
      "why_it_matters": "Infra_Observability Red; unstructured logs block automated alerting; no JSON logging found",
      "evidence_anchor": ["tests/infra/test_structured_logging.py", "infrastructure/logging.py", "agents/*.py"],
      "acceptance_signal": "Test verifies error logs are valid JSON; log aggregator parses 100%; metric log_parse_errors=0",
      "urgency": "P1",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_076",
      "area": "CI/CD",
      "question": "What prevents deploy if coverage <80% on critical components?",
      "why_it_matters": "CI_CD Red; low coverage on C01/C05/C10/C11/C12 risks; no coverage gate found",
      "evidence_anchor": [".github/workflows/ci.yaml", "tests/coverage_report.py"],
      "acceptance_signal": "CI fails if critical component coverage <80%; metric critical_coverage_percent >80; deploy blocked",
      "urgency": "P1",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_077",
      "area": "Data",
      "question": "How is schema version registry persistence tested across restarts?",
      "why_it_matters": "Q_029/Q_025 done; schema registry loss causes v19 fallback failure; no persistence test found",
      "evidence_anchor": ["tests/agents/test_schema_registry_persistence.py", "agents/schema_validator.py"],
      "acceptance_signal": "Test verifies registry persists; schema versions restored after restart; metric schema_registry_load_success=1",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_078",
      "area": "Infra",
      "question": "What health check endpoint exposes component status for load balancer?",
      "why_it_matters": "Infra reliability; no /health endpoint found; LB can't route around failures",
      "evidence_anchor": ["tests/infra/test_health_endpoint.py", "api/health.py", "infrastructure/load_balancer.yaml"],
      "acceptance_signal": "Test verifies /health returns 200 when healthy; LB removes unhealthy instances; metric health_check_failures increments",
      "urgency": "P1",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_079",
      "area": "Data",
      "question": "How is GA4 BigQuery export schema drift detected and alerted?",
      "why_it_matters": "Q_003/Q_029 related; GA4 schema changes break MTA; no GA4 schema monitoring found",
      "evidence_anchor": ["tests/agents/test_ga4_schema_drift.py", "agents/analytics_agent.py", "agents/schema_validator.py"],
      "acceptance_signal": "Test verifies GA4 schema change triggers alert <5min; metric ga4_schema_drift_detected increments; MTA paused on drift",
      "urgency": "P1",
      "owner_hint": "DataOps"
    }
  ]
}
