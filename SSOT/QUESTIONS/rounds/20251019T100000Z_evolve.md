{
  "questions": [
    {
      "id": "Q_201",
      "area": "Data",
      "question": "Event_id dedup requires atomic check-and-set; is graph_builder.py using optimistic locks or external mutex?",
      "why_it_matters": "C01 Red/CRITICAL Q_001/Q_101 in_progress need impl; concurrent creates may race without proper locking",
      "evidence_anchor": ["core/graph_builder.py", "agents/identity_resolution_agent.py"],
      "acceptance_signal": "Lock type documented in graph_builder, unit test confirms no dup event_id under concurrent load",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_202",
      "area": "Secrets",
      "question": "Salt rotation: after rotate_salt(), do we invalidate cached user_keys or require graph rebuild?",
      "why_it_matters": "C01 Red/CRITICAL Q_102 in_progress; stale keys post-rotation → identity resolution errors",
      "evidence_anchor": ["core/privacy.py", "core/graph_builder.py"],
      "acceptance_signal": "Test: rotate_salt() → resolve_identity uses new salt immediately (no cache hit on old key)",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_203",
      "area": "Risk",
      "question": "MMM MAPE >15% → reject; do we also gate on posterior credible interval width (e.g., CI width <X)?",
      "why_it_matters": "C04 Red/HIGH Q_009 in_progress; low MAPE but wide CI → unreliable predictions",
      "evidence_anchor": ["agents/mmm_agent.py", "tests/agents/test_mmm_agent.py"],
      "acceptance_signal": "Test: MAPE=14% + CI_width=0.9 → validation pass if width <threshold, else reject",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_204",
      "area": "Data",
      "question": "MTA path length cap: do we drop paths >10 touches silently or emit metric for monitoring?",
      "why_it_matters": "C05 Red/CRITICAL Q_010 open; silent drops → invisible data loss, hard to debug",
      "evidence_anchor": ["agents/mta_agent.py", "core/metrics.py"],
      "acceptance_signal": "Metric mta_paths_dropped_total increments on >10 touch drop",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_205",
      "area": "Compliance",
      "question": "MTA k-anonymity suppression: do we emit audit event with suppressed_path_count for compliance reporting?",
      "why_it_matters": "C05 Red/CRITICAL Q_044 open; privacy suppression must be auditable for GDPR",
      "evidence_anchor": ["agents/mta_agent.py", "core/audit.py"],
      "acceptance_signal": "Audit log contains k_anonymity_suppressed event with count when k<10",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_206",
      "area": "Risk",
      "question": "LLM temperature guard: does middleware/llm_guard.py validate before OR after prompt enrichment?",
      "why_it_matters": "C10 Red/CRITICAL Q_015 open; post-enrichment check → injection may bypass",
      "evidence_anchor": ["middleware/llm_guard.py", "agents/llm_council.py"],
      "acceptance_signal": "Test: enrich adds temp param → guard rejects before LLM call",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_207",
      "area": "Risk",
      "question": "LLM <2 sources reject: do we count distinct sources or distinct URLs (same domain = 1 source)?",
      "why_it_matters": "C10 Red/CRITICAL Q_016 open; URL-level counting → 5 URLs from 1 domain may pass incorrectly",
      "evidence_anchor": ["agents/llm_council.py", "agents/retriever.py"],
      "acceptance_signal": "Test: 3 URLs from 'example.com' + 0 others → reject (distinct_sources=1<2)",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_208",
      "area": "Secrets",
      "question": "Verifier LLM separate API key: do we rotate both Analyst and Verifier keys on same schedule or independent?",
      "why_it_matters": "C10 Red/CRITICAL Q_017 open; same schedule → both expire simultaneously (outage risk)",
      "evidence_anchor": ["agents/llm_council.py", "core/security.py"],
      "acceptance_signal": "Config shows analyst_key_rotation=30d, verifier_key_rotation=45d (independent)",
      "urgency": "P0",
      "owner_hint": "Secrets"
    },
    {
      "id": "Q_209",
      "area": "Risk",
      "question": "Crisis risk cap: if no official domain match, do we also check for â‰¥2 independent tier-1 sources (e.g., Reuters)?",
      "why_it_matters": "C11 Red/CRITICAL Q_018 open; cap=0.5 alone may miss legit tier-1 corroboration",
      "evidence_anchor": ["agents/crisis_detection_agent.py", "config/trusted_sources.yml"],
      "acceptance_signal": "Test: no official + 2 tier-1 sources → risk ≤0.6 (relaxed cap)",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_210",
      "area": "Observability",
      "question": "Crisis velocity 3-sigma: do we log baseline mean/std in metrics for drift detection over time?",
      "why_it_matters": "C11 Red/CRITICAL Q_019/Q_054 open; static sigma → threshold drift undetectable",
      "evidence_anchor": ["agents/crisis_detection_agent.py", "core/metrics.py"],
      "acceptance_signal": "Metric crisis_velocity_baseline_mean/std exported with 24h window",
      "urgency": "P0",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_211",
      "area": "Compliance",
      "question": "COPPA age gate: do we require parent email validation OR just DOB + self-attestation checkbox?",
      "why_it_matters": "C12 Red/CRITICAL Q_020/Q_056 open; dual validation unclear → COPPA violation risk",
      "evidence_anchor": ["middleware/compliance_guard.py", "schemas/user.zod.ts"],
      "acceptance_signal": "Test: age<13 + no parent_email → reject; age<13 + parent_email + checkbox → pass",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_212",
      "area": "Compliance",
      "question": "Japan promo label: do we validate label exists in rendered creative OR only in metadata field?",
      "why_it_matters": "C12 Red/CRITICAL Q_021 open; metadata-only check → visual may miss label (non-compliance)",
      "evidence_anchor": ["agents/compliance_agent.py", "agents/creative_intelligence_agent.py"],
      "acceptance_signal": "Test: creative.text contains 'åºƒå'Š' substring + metadata.promo=true (both required)",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_213",
      "area": "Risk",
      "question": "Budget allocation cap ±25%: is the cap per-channel absolute or relative to current spend?",
      "why_it_matters": "C13 Red/CRITICAL Q_022 open; absolute cap → small channels can't scale; relative → gaming risk",
      "evidence_anchor": ["agents/budget_allocation_agent.py", "schemas/budget_allocation.zod.ts"],
      "acceptance_signal": "Test: channel at 10K → new=35K rejected (250% change); channel at 100K → new=124K passes",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_214",
      "area": "Data",
      "question": "Allocation hash ID: do we include timestamp in hash or only allocation params (for idempotency)?",
      "why_it_matters": "C13 Red/CRITICAL Q_023 open; timestamp-based → retries create dups; param-only → correct",
      "evidence_anchor": ["agents/budget_allocation_agent.py"],
      "acceptance_signal": "Test: identical params + different timestamps → same allocation_id (no timestamp in hash)",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_215",
      "area": "Risk",
      "question": "Pacing >110% pause: do we pause campaign-level or only overpaced assets?",
      "why_it_matters": "C14 Red/CRITICAL Q_024/Q_062 open; campaign-level → collateral damage; asset-level → precise",
      "evidence_anchor": ["agents/pacing_agent.py", "schemas/pacing_status.zod.ts"],
      "acceptance_signal": "Test: asset A at 120%, asset B at 80% → only A paused, B continues",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_216",
      "area": "Execution",
      "question": "Kill-switch <5s: do we measure from trigger() call to last API mutation confirmed OR to local state change?",
      "why_it_matters": "C17 Red/CRITICAL Q_028 open; local-only → mutations in-flight continue (not true kill)",
      "evidence_anchor": ["agents/activation_agent.py", "tests/agents/test_activation_agent.py"],
      "acceptance_signal": "Test: trigger kill → last mutation ACK timestamp - trigger_timestamp <5s",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_217",
      "area": "Data",
      "question": "Activation dedup: do we store idempotency keys in Redis with TTL or Postgres forever?",
      "why_it_matters": "C17 Red/CRITICAL Q_029 open; Redis TTL → keys expire, old retries may dup; Postgres → disk bloat",
      "evidence_anchor": ["agents/activation_agent.py", "core/redis.py"],
      "acceptance_signal": "Config shows idempotency_ttl=7d in Redis; test confirms 8d retry → new mutation (key expired)",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_218",
      "area": "Observability",
      "question": "Activation backoff: do we export metric with retry_attempt number for SLO tracking?",
      "why_it_matters": "C17 Red/CRITICAL Q_069 open; no retry metrics → can't measure 99p latency impact of retries",
      "evidence_anchor": ["agents/activation_agent.py", "core/metrics.py"],
      "acceptance_signal": "Metric activation_retry_total{attempt='1|2|3'} increments on backoff",
      "urgency": "P0",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_219",
      "area": "Execution",
      "question": "Activation timeout 30s: do we rollback in-flight mutations on timeout or leave partial state?",
      "why_it_matters": "C17 Red/CRITICAL Q_070 open; partial state → inconsistent budgets, hard rollback later",
      "evidence_anchor": ["agents/activation_agent.py", "schemas/mutation_result.zod.ts"],
      "acceptance_signal": "Test: 5 mutations planned, 3 succeed before 30s timeout → rollback all 3 (atomic batch)",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_220",
      "area": "Observability",
      "question": "Audit S3 fallback: do we emit metric audit_fallback_active when DB down?",
      "why_it_matters": "C20 Red/HIGH Q_078 open; silent fallback → ops unaware of DB issues",
      "evidence_anchor": ["core/audit.py", "core/metrics.py"],
      "acceptance_signal": "Metric audit_fallback_active=1 when S3 mode enabled",
      "urgency": "P0",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_221",
      "area": "Compliance",
      "question": "Audit HMAC: do we rotate HMAC keys periodically or use single long-lived key?",
      "why_it_matters": "C20 Red/HIGH Q_079 open; single key forever → compromise risk; rotation → old audits unverifiable",
      "evidence_anchor": ["core/audit.py", "core/security.py"],
      "acceptance_signal": "Config shows hmac_key_rotation=90d; old audits include key_version for multi-key validation",
      "urgency": "P0",
      "owner_hint": "Secrets"
    },
    {
      "id": "Q_222",
      "area": "Observability",
      "question": "P0→PagerDuty <2min: do we measure from metric threshold breach OR alert fired?",
      "why_it_matters": "C27 Red/CRITICAL Q_144 open; threshold→PD includes alert eval delay (more accurate SLO)",
      "evidence_anchor": ["core/alerting.py", "infrastructure/alertmanager.yaml"],
      "acceptance_signal": "Trace shows threshold_breach_timestamp → pagerduty_api_success_timestamp <120s",
      "urgency": "P0",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_223",
      "area": "CI/CD",
      "question": "Feature parity CI: do we fail on >1% OR emit warning + require manual approval?",
      "why_it_matters": "C03 Yellow Q_003 open; auto-fail → blocks deploys; warning-only → drift creeps",
      "evidence_anchor": ["tests/test_feature_parity.py", ".github/workflows/ci.yml"],
      "acceptance_signal": "CI config shows parity_check: fail_threshold=0.01 (auto-fail mode)",
      "urgency": "P1",
      "owner_hint": "CI/CD"
    },
    {
      "id": "Q_224",
      "area": "Data",
      "question": "Feature unique constraint: DB-level UNIQUE(entity,feature,ts) or app-level check before insert?",
      "why_it_matters": "C03 Yellow Q_004 open; app-only → race conditions; DB-level → proper enforcement",
      "evidence_anchor": ["core/feature_store.py", "migrations/"],
      "acceptance_signal": "Migration adds UNIQUE constraint; test confirms duplicate insert → DB IntegrityError",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_225",
      "area": "Risk",
      "question": "LLM injection detect: do we validate input prompts or only output responses?",
      "why_it_matters": "C10 Red/CRITICAL Q_051 open; output-only → injection in system prompt bypasses guard",
      "evidence_anchor": ["middleware/llm_guard.py"],
      "acceptance_signal": "Test: user_input contains 'ignore previous' → rejected before prompt enrichment",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_226",
      "area": "Risk",
      "question": "LLM toxicity >0.8: do we check Analyst output, Verifier output, or both?",
      "why_it_matters": "C10 Red/CRITICAL Q_052 open; Analyst-only → Verifier toxic pass-through risk",
      "evidence_anchor": ["agents/llm_council.py", "agents/verifier_llm.py"],
      "acceptance_signal": "Test: Analyst toxic → reject; Verifier toxic → reject; both checked independently",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_227",
      "area": "Compliance",
      "question": "Domain whitelist sync: do we reload config on-demand or require service restart?",
      "why_it_matters": "C11 Red/CRITICAL Q_055 open; restart-only → config lag during crisis",
      "evidence_anchor": ["config/official_domains.yml", "agents/crisis_detection_agent.py"],
      "acceptance_signal": "Test: update config → agent reloads within 60s (no restart)",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_228",
      "area": "Compliance",
      "question": "GDPR cascade delete: do we also delete from feature_store or only audit/user tables?",
      "why_it_matters": "C12 Red/CRITICAL Q_057 open; feature_store retained → GDPR violation (user data remains)",
      "evidence_anchor": ["core/gdpr.py", "core/feature_store.py"],
      "acceptance_signal": "Test: GDPR delete → user_key removed from all tables including features within 30d",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_229",
      "area": "Risk",
      "question": "Medical claims filter: do we block only exact matches or also semantic similarity (e.g., 'heals' ~ 'cures')?",
      "why_it_matters": "C12 Red/CRITICAL Q_058 open; exact-only → easy bypass with synonyms",
      "evidence_anchor": ["agents/compliance_agent.py", "config/banned_claims.yml"],
      "acceptance_signal": "Test: 'heals disease' → rejected (semantic match to 'cure' ban)",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_230",
      "area": "Execution",
      "question": "Budget LWW conflict: do we use server timestamp or client-submitted timestamp for last-write?",
      "why_it_matters": "C13 Red/CRITICAL Q_060 open; client timestamp → clock skew gaming; server → correct",
      "evidence_anchor": ["agents/budget_allocation_agent.py"],
      "acceptance_signal": "Test: concurrent writes → server_timestamp determines winner (client timestamp ignored)",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_231",
      "area": "Governance",
      "question": "Approval 24h timeout: do we auto-reject or route to backup approver on timeout?",
      "why_it_matters": "A_011 P1 MINOR open; auto-reject → blocks urgent changes; backup → continuity",
      "evidence_anchor": ["agents/approval_agent.py", "config/approval_rules.yml"],
      "acceptance_signal": "Config shows timeout_action='route_backup'; test confirms 25h → backup notified",
      "urgency": "P1",
      "owner_hint": "Governance"
    },
    {
      "id": "Q_232",
      "area": "Data",
      "question": "Identity graph merge: do we preserve merge history for audit or only final state?",
      "why_it_matters": "C01 in_progress; no history → can't debug bad merges or GDPR right-to-explanation",
      "evidence_anchor": ["core/graph_builder.py", "core/audit.py"],
      "acceptance_signal": "Audit log contains profile_merged event with old_user_key + new_user_key + timestamp",
      "urgency": "P1",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_233",
      "area": "Risk",
      "question": "MMM posterior sampling: do we fix random seed for reproducibility or use system entropy?",
      "why_it_matters": "C04 in_progress; no seed → A/B test results non-reproducible (debugging nightmare)",
      "evidence_anchor": ["agents/mmm_agent.py"],
      "acceptance_signal": "Config shows pymc_random_seed=42; test confirms identical data → identical posteriors",
      "urgency": "P1",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_234",
      "area": "Observability",
      "question": "MMM training duration: do we emit metric for model fit time to detect regressions?",
      "why_it_matters": "C04 in_progress; slow fit → weekly retrain SLA miss",
      "evidence_anchor": ["agents/mmm_agent.py", "core/metrics.py"],
      "acceptance_signal": "Metric mmm_train_duration_seconds histogram exported",
      "urgency": "P1",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_235",
      "area": "Data",
      "question": "MTA path aggregation: do we use deterministic hash or random sampling for privacy?",
      "why_it_matters": "C05 Red/CRITICAL; deterministic → same user always same bucket (linkability); random → safer",
      "evidence_anchor": ["agents/mta_agent.py"],
      "acceptance_signal": "Code shows random.choice() not hash-based bucketing",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_236",
      "area": "Risk",
      "question": "Creative fatigue detection: do we use linear regression or exponential decay model?",
      "why_it_matters": "C08 Red; linear → misses saturation curve; exponential → better fit",
      "evidence_anchor": ["agents/creative_intelligence_agent.py"],
      "acceptance_signal": "Test: CTR plateau then drop → exponential model detects earlier than linear",
      "urgency": "P1",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_237",
      "area": "Governance",
      "question": "Playbook execution: do we require approval BEFORE each step or only once at start?",
      "why_it_matters": "C16 Red/HIGH; per-step approval → safer but slow; start-only → fast but risky",
      "evidence_anchor": ["agents/playbook_orchestrator.py", "schemas/playbook.yml"],
      "acceptance_signal": "Playbook schema shows approval_mode='once_at_start' (configurable per playbook)",
      "urgency": "P1",
      "owner_hint": "Governance"
    },
    {
      "id": "Q_238",
      "area": "Observability",
      "question": "Distributed tracing: do we propagate trace context through Kafka or only HTTP?",
      "why_it_matters": "Infra_Observability Green but may lack async; no Kafka trace → broken traces",
      "evidence_anchor": ["core/tracing.py", "core/message_bus.py"],
      "acceptance_signal": "Test: HTTP→Kafka→HTTP flow has single continuous trace_id",
      "urgency": "P1",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_239",
      "area": "RBAC",
      "question": "RBAC policy changes: do we require approval OR auto-apply with audit log?",
      "why_it_matters": "Infra_Auth Green but RBAC drift risk; no approval → accidental over-privilege",
      "evidence_anchor": ["infrastructure/api_gateway.py", "config/rbac_policies.yml"],
      "acceptance_signal": "Config shows rbac_change_approval=true; test confirms policy edit → approval flow",
      "urgency": "P1",
      "owner_hint": "Governance"
    },
    {
      "id": "Q_240",
      "area": "Secrets",
      "question": "JWT expiry: do we use short-lived (15min) with refresh tokens or long-lived (24h)?",
      "why_it_matters": "Infra_Auth Green but token lifetime unclear; long-lived → higher compromise window",
      "evidence_anchor": ["infrastructure/api_gateway.py", "core/security.py"],
      "acceptance_signal": "Config shows jwt_ttl=900s (15min) + refresh_token_ttl=7d",
      "urgency": "P1",
      "owner_hint": "Secrets"
    },
    {
      "id": "Q_241",
      "area": "Risk",
      "question": "Circuit breaker: do we fail open or closed when state store (Redis) is down?",
      "why_it_matters": "Infra_ExternalAPIs Green but fail mode unclear; fail-closed → full outage; fail-open → risk",
      "evidence_anchor": ["core/circuit_breaker.py"],
      "acceptance_signal": "Code shows fail_open=false (fail-closed by default); test confirms Redis down → 503",
      "urgency": "P1",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_242",
      "area": "Execution",
      "question": "Rate limiter: do we use token bucket or leaky bucket algorithm?",
      "why_it_matters": "Infra_ExternalAPIs Green but algo unclear; leaky → smoother; token → bursty",
      "evidence_anchor": ["core/rate_limiter.py"],
      "acceptance_signal": "Code shows token bucket with refill_rate config",
      "urgency": "P2",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_243",
      "area": "Observability",
      "question": "Metric cardinality: do we enforce label cardinality limits to prevent Prometheus explosion?",
      "why_it_matters": "Infra_Observability Green but high-cardinality labels → memory/query issues",
      "evidence_anchor": ["core/metrics.py", "infrastructure/prometheus.yaml"],
      "acceptance_signal": "Config shows max_label_cardinality=1000; test confirms 1001st unique label → dropped",
      "urgency": "P1",
      "owner_hint": "Observability"
    },
    {
      "id": "Q_244",
      "area": "CI/CD",
      "question": "Deploy rollback: do we support automatic rollback on failed health checks or only manual?",
      "why_it_matters": "CI_CD Green but rollback unclear; manual-only → outage during off-hours",
      "evidence_anchor": [".github/workflows/deploy.yaml", "infrastructure/k8s/deployment.yaml"],
      "acceptance_signal": "Workflow shows auto_rollback=true on readiness probe failure",
      "urgency": "P1",
      "owner_hint": "CI/CD"
    },
    {
      "id": "Q_245",
      "area": "Secrets",
      "question": "Secret rotation: do we support zero-downtime rotation with dual-key overlap window?",
      "why_it_matters": "DataOps_Secrets Green but rotation method unclear; no overlap → brief outage",
      "evidence_anchor": ["core/security.py"],
      "acceptance_signal": "Test: rotate key → both old and new valid for 5min grace window",
      "urgency": "P1",
      "owner_hint": "Secrets"
    },
    {
      "id": "Q_246",
      "area": "Data",
      "question": "Revenue attribution: do we reconcile MMM + MTA totals with actual revenue or allow divergence?",
      "why_it_matters": "DataOps_Revenue Red; no reconciliation → undetected double-count or leak",
      "evidence_anchor": ["agents/mmm_agent.py", "agents/mta_agent.py", "core/feature_store.py"],
      "acceptance_signal": "Daily job checks |MMM_total + MTA_total - actual_revenue| <5%; alert if exceeded",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_247",
      "area": "Data",
      "question": "Schema drift: do we use JSON Schema version field or implicit versioning by timestamp?",
      "why_it_matters": "DataOps_Validation Red; implicit versioning → hard to correlate schema with data",
      "evidence_anchor": ["schemas/*.zod.ts", "core/contracts.py"],
      "acceptance_signal": "All schemas include version='1.0.0' field; test confirms version mismatch → validation error",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_248",
      "area": "Risk",
      "question": "Null hypothesis testing: do we use two-tailed or one-tailed tests for A/B experiments?",
      "why_it_matters": "C04/C05 statistical tests unclear; one-tailed → assumes direction (dangerous)",
      "evidence_anchor": ["agents/ab_test_agent.py"],
      "acceptance_signal": "Code shows scipy.stats.ttest_ind(alternative='two-sided')",
      "urgency": "P1",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_249",
      "area": "Execution",
      "question": "Budget pacing: do we recalculate pacing target intraday or only at midnight?",
      "why_it_matters": "C14 Red/CRITICAL; midnight-only → morning overspend, evening underspend",
      "evidence_anchor": ["agents/pacing_agent.py"],
      "acceptance_signal": "Test: budget consumed 80% at noon → target recalculated for remaining hours",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_250",
      "area": "Observability",
      "question": "SLO tracking: do we export SLO compliance metrics or only raw latency/error rate?",
      "why_it_matters": "Monitoring_Alerting Red/CRITICAL; raw metrics → manual SLO calc (error-prone)",
      "evidence_anchor": ["core/metrics.py", "infrastructure/prometheus.yaml"],
      "acceptance_signal": "Metric slo_compliance_percent{slo='p99_latency'} exported with 0-100 value",
      "urgency": "P0",
      "owner_hint": "Observability"
    }
  ]
}
