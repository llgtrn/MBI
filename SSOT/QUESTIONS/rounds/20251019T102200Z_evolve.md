{
  "questions": [
    {
      "id": "Q_351",
      "area": "Data",
      "question": "How do we prevent stale feature values when feature store write fails but online cache succeeds?",
      "why_it_matters": "C03 Yellow; online/offline parity Q_005 requires atomic write-through; cache inconsistency causes model drift",
      "evidence_anchor": ["agents/feature_engineering_agent.py", "core/feature_store.py"],
      "acceptance_signal": "Test: mock feature_store.write() failure→cache rollback or lock prevents reads; metric feature_write_failures_total",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_352",
      "area": "Data",
      "question": "What TTL do we set on feature_store online cache to bound staleness when offline writes are delayed?",
      "why_it_matters": "C03 Yellow; Q_005 parity requires bounded staleness; long TTL amplifies offline lag impact",
      "evidence_anchor": ["core/feature_store.py", "config/feature_ttl.yml"],
      "acceptance_signal": "Config: default TTL ≤60s; test: offline write delayed 65s→online fetch returns null",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_353",
      "area": "Data",
      "question": "Do we validate feature schema compatibility on writes to prevent downstream model breakage?",
      "why_it_matters": "C03 Yellow; feature drift breaks model contracts; current coverage missing contract validation",
      "evidence_anchor": ["core/feature_store.py", "schemas/feature_schema.json"],
      "acceptance_signal": "Test: write feature with incompatible type→ValidationError; schema_version increments",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_354",
      "area": "Risk",
      "question": "How do we detect and mitigate MMM overfitting when R² >0.95 but out-of-sample MAPE >20%?",
      "why_it_matters": "C04 Red; Q_009 MAPE gate doesn't catch overfitting; high in-sample R² masks poor generalization",
      "evidence_anchor": ["agents/mmm_agent.py", "tests/agents/test_mmm_agent.py"],
      "acceptance_signal": "Test: train on synthetic overfit data→validation rejects high R² + high holdout MAPE",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_355",
      "area": "Risk",
      "question": "What is our MMM retraining frequency and do we auto-reject stale models >8 weeks old?",
      "why_it_matters": "C04 Red; weekly schedule from AGENT.md; stale models miss seasonality/market shifts",
      "evidence_anchor": ["agents/mmm_agent.py", "config/model_lifecycle.yml"],
      "acceptance_signal": "Config: max_age_days=56; test: model trained 57d ago→load_latest_model() raises StaleModelError",
      "urgency": "P1",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_356",
      "area": "Risk",
      "question": "Do we compare MMM and MTA attribution and alert when divergence >30% for any channel?",
      "why_it_matters": "C04/C05 Red; reconciliation required per AGENT.md; large divergence indicates measurement error",
      "evidence_anchor": ["agents/mmm_agent.py", "agents/mta_agent.py", "core/alerting.py"],
      "acceptance_signal": "Metric: attribution_divergence_pct{channel}; alert if any >30%; test: mock 35% divergence→P1 alert",
      "urgency": "P1",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_357",
      "area": "Data",
      "question": "How do we handle MTA conversion paths that span salt rotation and invalidate old user_key hashes?",
      "why_it_matters": "C05 Red; Q_102 salt rotation breaks path continuity; orphaned hashes cause attribution loss",
      "evidence_anchor": ["agents/mta_agent.py", "core/privacy.py"],
      "acceptance_signal": "Test: path starts pre-rotation, ends post-rotation→path reconstructed using key_version mapping",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_358",
      "area": "Data",
      "question": "What is our MTA path reconstruction lag after identity resolution completes?",
      "why_it_matters": "C05 Red; late reconstruction delays attribution reports; SLA impacts decisioning",
      "evidence_anchor": ["agents/mta_agent.py", "agents/identity_resolution_agent.py"],
      "acceptance_signal": "Metric: mta_path_lag_seconds p95; alert if >300s; test: mock identity event→path ready <5min",
      "urgency": "P1",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_359",
      "area": "Risk",
      "question": "Do we audit and log every k-anonymity suppression decision in MTA for GDPR compliance?",
      "why_it_matters": "C05 Red; Q_044 k>=10 audit requirement; unaudited suppressions violate transparency",
      "evidence_anchor": ["agents/mta_agent.py", "core/audit.py"],
      "acceptance_signal": "Audit: mta_kanonymity_suppressed with path_hash, k_value, suppressed_count; test: k=7→audit emitted",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_360",
      "area": "Strategy",
      "question": "How do we validate geo-experiment market pairs are truly matched before running incrementality tests?",
      "why_it_matters": "C06 Red; unmatched markets confound treatment effect; current spec lacks pre-flight validation",
      "evidence_anchor": ["agents/incrementality_agent.py"],
      "acceptance_signal": "Test: design_geo_experiment with dissimilar markets→raises ValidationError; match_score <0.7 rejects",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_361",
      "area": "Risk",
      "question": "What is our incrementality test minimum runtime to achieve 80% statistical power?",
      "why_it_matters": "C06 Red; 4-week default from spec may be insufficient for low-volume markets",
      "evidence_anchor": ["agents/incrementality_agent.py", "config/experiment_config.yml"],
      "acceptance_signal": "Config: min_runtime_days by market tier; test: mock low-volume→requires ≥6 weeks",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_362",
      "area": "Data",
      "question": "Do we normalize brand mention volume by platform reach to prevent Twitter overweighting in buzz metrics?",
      "why_it_matters": "C07 Red; raw counts bias toward high-volume platforms; reach normalization required for fair SOV",
      "evidence_anchor": ["agents/brand_tracking_agent.py"],
      "acceptance_signal": "Test: 1000 Twitter mentions + 100 Instagram mentions→weighted score accounts for platform reach",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_363",
      "area": "Risk",
      "question": "How do we prevent spam/bot accounts from inflating brand sentiment scores?",
      "why_it_matters": "C07 Red; bot manipulation distorts sentiment; current spec lacks anti-spam filtering",
      "evidence_anchor": ["agents/brand_tracking_agent.py", "middleware/spam_filter.py"],
      "acceptance_signal": "Test: inject synthetic bot mentions→filtered out; metric spam_filtered_count; bot_score threshold 0.8",
      "urgency": "P1",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_364",
      "area": "Risk",
      "question": "What is our creative fatigue score recalibration frequency to adapt to market velocity changes?",
      "why_it_matters": "C08 Red; static 0.7 threshold from spec may miss fast-moving trends; periodic recalibration required",
      "evidence_anchor": ["agents/creative_intelligence_agent.py", "config/fatigue_config.yml"],
      "acceptance_signal": "Config: recalibrate_every_days=30; test: mock CTR trend shift→threshold adjusted",
      "urgency": "P2",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_365",
      "area": "Data",
      "question": "Do we track creative asset lineage to understand which variants descend from which base assets?",
      "why_it_matters": "C08 Red; variant generation lacks provenance tracking; can't attribute performance to motifs",
      "evidence_anchor": ["agents/creative_intelligence_agent.py", "schemas/creative_asset.json"],
      "acceptance_signal": "Schema: parent_asset_id field; test: generate_variants→child assets link to parent",
      "urgency": "P2",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_366",
      "area": "Risk",
      "question": "How do we prevent LLM creative variants from leaking competitor brand names into our ad copy?",
      "why_it_matters": "C08 Red; brand confusion violates ad platform policies; legal risk",
      "evidence_anchor": ["agents/creative_intelligence_agent.py", "agents/compliance_agent.py"],
      "acceptance_signal": "Test: RAG includes competitor mention→generated variant scrubs competitor name",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_367",
      "area": "Strategy",
      "question": "What minimum cluster size do we enforce in audience clustering to ensure stable segments?",
      "why_it_matters": "C09 Red; tiny clusters (<100) cause high variance in targeting; current spec lacks size gate",
      "evidence_anchor": ["agents/audience_clustering_agent.py", "config/clustering_config.yml"],
      "acceptance_signal": "Config: min_cluster_size=1000; test: clustering output→all clusters ≥1000 users",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_368",
      "area": "Risk",
      "question": "Do we validate audience clusters don't inadvertently create discriminatory segments based on protected attributes?",
      "why_it_matters": "C09 Red; unsupervised clustering may proxy for race/gender; compliance risk",
      "evidence_anchor": ["agents/audience_clustering_agent.py", "middleware/fairness_check.py"],
      "acceptance_signal": "Test: clustering with demographic features→fairness_check() rejects if proxy detected",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_369",
      "area": "Risk",
      "question": "How do we prevent LLM temperature drift when model provider updates their API defaults?",
      "why_it_matters": "C10 Red; Q_015 gate assumes temp≤0.2; provider default change silently bypasses guard",
      "evidence_anchor": ["agents/llm_council.py", "middleware/llm_guard.py"],
      "acceptance_signal": "Test: mock API returns temp=0.3 despite our request→pre-call validation rejects",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_370",
      "area": "Risk",
      "question": "What is our LLM request/response payload size limit to prevent DoS via large context injection?",
      "why_it_matters": "C10 Red; unlimited payload allows cost/latency attacks; current spec lacks size gate",
      "evidence_anchor": ["middleware/llm_guard.py", "config/llm_limits.yml"],
      "acceptance_signal": "Config: max_prompt_tokens=8000, max_response_tokens=2000; test: oversized→HTTP 413",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_371",
      "area": "Risk",
      "question": "Do we validate LLM RAG source_ids actually exist in SSOT before accepting the response?",
      "why_it_matters": "C10 Red; Q_016 requires ≥2 sources; LLM could hallucinate source_ids",
      "evidence_anchor": ["agents/llm_council.py", "core/feature_store.py"],
      "acceptance_signal": "Test: LLM returns source_ids=['fake_1','fake_2']→verifier rejects; query SSOT for each ID",
      "urgency": "P0",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_372",
      "area": "Observability",
      "question": "How do we trace LLM Council end-to-end latency breakdown (Retriever→Analyst→Verifier)?",
      "why_it_matters": "C10 Red; no observability per coverage matrix; can't optimize bottlenecks",
      "evidence_anchor": ["agents/llm_council.py", "core/tracing.py"],
      "acceptance_signal": "Metric: llm_council_latency_seconds{stage}; trace spans for each stage; p95 <5s total",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_373",
      "area": "Risk",
      "question": "What is our crisis detection false positive rate target and how do we measure it?",
      "why_it_matters": "C11 Red; 3σ gate Q_054 may trigger too often; need empirical FP baseline",
      "evidence_anchor": ["agents/crisis_detection_agent.py", "tests/agents/test_crisis_detection_agent.py"],
      "acceptance_signal": "Metric: crisis_false_positive_rate; target <5%; test: inject benign spike→no P0 alert",
      "urgency": "P1",
      "owner_hint": "Risk"
    },
    {
      "id": "Q_374",
      "area": "Risk",
      "question": "Do we auto-suppress crisis alerts during known campaign launches to prevent launch-driven false alarms?",
      "why_it_matters": "C11 Red; planned campaigns cause velocity spikes; current spec lacks suppression window",
      "evidence_anchor": ["agents/crisis_detection_agent.py", "core/campaign_calendar.py"],
      "acceptance_signal": "Config: campaign_launch_suppress_hours=24; test: spike during launch→risk_score capped at 0.4",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_375",
      "area": "Compliance",
      "question": "How do we verify crisis detection official domain whitelist is synced across all regions?",
      "why_it_matters": "C11 Red; Q_055 reload <60s; multi-region deployment requires consistent whitelist",
      "evidence_anchor": ["config/official_domains.yml", "agents/crisis_detection_agent.py"],
      "acceptance_signal": "Test: update domain in region A→region B sees change <60s; metric config_sync_lag_seconds",
      "urgency": "P0",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_376",
      "area": "Compliance",
      "question": "What is our COPPA parental consent email verification expiry and do we re-verify periodically?",
      "why_it_matters": "C12 Red; Q_020 dual validation; one-time consent insufficient if email changes",
      "evidence_anchor": ["middleware/compliance_guard.py", "config/coppa_config.yml"],
      "acceptance_signal": "Config: consent_expiry_days=365; test: expired consent→requires re-verification",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_377",
      "area": "Compliance",
      "question": "Do we validate promo labeling presence in all supported languages including CJK character sets?",
      "why_it_matters": "C12 Red; Q_021 Japan promo visual; current regex may miss full-width characters",
      "evidence_anchor": ["agents/compliance_agent.py", "config/promo_labels.json"],
      "acceptance_signal": "Test: Japanese text 'ï¼ˆåºƒå'Šï¼‰'→detected; Chinese 'ï¼ˆæŽ¨å¹¿ï¼‰'→detected; regex supports Unicode",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_378",
      "area": "Compliance",
      "question": "How do we audit GDPR deletion requests to prove feature cascade completed within 30 days?",
      "why_it_matters": "C12 Red; Q_057 cascade requirement; audit trail required for regulatory compliance",
      "evidence_anchor": ["core/gdpr.py", "core/audit.py"],
      "acceptance_signal": "Audit: gdpr_deletion_completed with user_key, deleted_rows, completion_time <30d",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_379",
      "area": "Compliance",
      "question": "What medical claims semantic filter recall rate do we target to avoid rejecting legitimate health content?",
      "why_it_matters": "C12 Red; Q_058 semantic filter; over-aggressive blocking harms campaigns",
      "evidence_anchor": ["agents/compliance_agent.py", "tests/agents/test_compliance_agent.py"],
      "acceptance_signal": "Test: 100 labeled health examples→recall ≥95%; precision ≥90%; F1 ≥0.92",
      "urgency": "P1",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_380",
      "area": "Risk",
      "question": "How do we prevent budget allocation hash collisions when multiple users concurrently submit identical requests?",
      "why_it_matters": "C13 Red; Q_023 hash ID collision risk; concurrent requests could generate same ID",
      "evidence_anchor": ["agents/budget_allocation_agent.py"],
      "acceptance_signal": "Code: hash includes timestamp + user_id; test: concurrent identical requests→distinct allocation_ids",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_381",
      "area": "Risk",
      "question": "Do we validate budget allocation sums to 100% before committing to prevent under/over-allocation?",
      "why_it_matters": "C13 Red; floating point rounding errors could cause allocation drift",
      "evidence_anchor": ["agents/budget_allocation_agent.py"],
      "acceptance_signal": "Test: allocations sum to 99.98%→rejected; sum 100.02%→rejected; tolerance ±0.01%",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_382",
      "area": "Risk",
      "question": "What is our budget cap breach notification latency target from detection to ops team alert?",
      "why_it_matters": "C13 Red; Q_328 audit logs breach; notification lag increases overspend risk",
      "evidence_anchor": ["agents/budget_allocation_agent.py", "core/alerting.py"],
      "acceptance_signal": "Metric: budget_breach_alert_latency_seconds p95 <300s; test: mock breach→Slack <5min",
      "urgency": "P1",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_383",
      "area": "Risk",
      "question": "Do we implement pacing pause with exponential backoff to prevent rapid on/off cycling?",
      "why_it_matters": "C14 Red; immediate re-enable after pause could cause oscillation; backoff stabilizes",
      "evidence_anchor": ["agents/pacing_agent.py", "config/pacing_config.yml"],
      "acceptance_signal": "Config: pause_backoff_minutes=[15,30,60]; test: 3 pauses in 1h→3rd pause lasts 60min",
      "urgency": "P1",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_384",
      "area": "Observability",
      "question": "How do we track pacing agent decision latency from breach detection to ad platform pause?",
      "why_it_matters": "C14 Red; Q_330 latency metric required; long latency increases overspend",
      "evidence_anchor": ["agents/pacing_agent.py", "core/metrics.py"],
      "acceptance_signal": "Metric: pacing_pause_latency_seconds{campaign_id} p95; test: mock breach→pause <60s",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_385",
      "area": "Risk",
      "question": "What is our creative rotation minimum runtime before declaring fatigue to avoid premature pausing?",
      "why_it_matters": "C15 Red; 0.7 fatigue threshold could trigger on launch spike; need minimum n impressions",
      "evidence_anchor": ["agents/creative_rotation_agent.py", "config/fatigue_config.yml"],
      "acceptance_signal": "Config: min_impressions=10000, min_days=3; test: 2d old asset→skip rotation",
      "urgency": "P1",
      "owner_hint": "Strategy"
    },
    {
      "id": "Q_386",
      "area": "Risk",
      "question": "Do we prevent creative rotation from pausing all assets simultaneously and causing campaign blackout?",
      "why_it_matters": "C15 Red; simultaneous pause breaks campaign delivery; need min active asset count",
      "evidence_anchor": ["agents/creative_rotation_agent.py"],
      "acceptance_signal": "Test: attempt to pause last 2 active assets→rejected; min_active_assets=2",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_387",
      "area": "RBAC",
      "question": "How do we handle approval workflow when approver is on PTO and fallback approver is also unavailable?",
      "why_it_matters": "C16 Red; A_011 24h timeout insufficient if all approvers unavailable; auto-escalation required",
      "evidence_anchor": ["agents/approval_agent.py", "config/approver_hierarchy.json"],
      "acceptance_signal": "Config: fallback_chain depth 3; test: all 3 unavailable→escalate to CMO",
      "urgency": "P1",
      "owner_hint": "RBAC"
    },
    {
      "id": "Q_388",
      "area": "Risk",
      "question": "Do we audit all approval actions including rejections and cancellations for compliance trail?",
      "why_it_matters": "C16 Red; current spec audits approvals only; rejections also require trail",
      "evidence_anchor": ["agents/approval_agent.py", "core/audit.py"],
      "acceptance_signal": "Audit: approval_action with status {approved|rejected|cancelled}, approver_id, reason",
      "urgency": "P1",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_389",
      "area": "Risk",
      "question": "How do we prevent activation retry storms when ad platform rate limits are hit?",
      "why_it_matters": "C17 Red; Q_333 backoff insufficient if multiple agents retry simultaneously; need global rate limit",
      "evidence_anchor": ["agents/activation_agent.py", "middleware/rate_limiter.py"],
      "acceptance_signal": "Test: 10 agents hit rate limit→global backoff applied; metric rate_limit_backoff_seconds",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_390",
      "area": "Risk",
      "question": "What is our activation idempotency key TTL and do we extend it on retry to prevent expiry mid-retry?",
      "why_it_matters": "C17 Red; Q_029 TTL=7d; long retry backoff could exceed TTL and cause duplicate mutations",
      "evidence_anchor": ["agents/activation_agent.py", "config/idempotency_config.yml"],
      "acceptance_signal": "Config: key_ttl_days=14, extend_on_retry=true; test: retry on day 6→TTL extended to day 20",
      "urgency": "P0",
      "owner_hint": "Execution"
    },
    {
      "id": "Q_391",
      "area": "Observability",
      "question": "How do we trace activation end-to-end including ad platform API response times?",
      "why_it_matters": "C17 Red; no observability per coverage matrix; can't diagnose external API slowness",
      "evidence_anchor": ["agents/activation_agent.py", "core/tracing.py"],
      "acceptance_signal": "Trace: spans for prepare→validate→push→confirm; metric external_api_latency_seconds{platform}",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_392",
      "area": "Compliance",
      "question": "Do we validate policy agent rule updates are backwards compatible before deployment?",
      "why_it_matters": "C18 Red; breaking rule changes could block valid campaigns; need compatibility check",
      "evidence_anchor": ["agents/policy_agent.py", "config/policy_rules.yml"],
      "acceptance_signal": "Test: new rule version→validate against 100 historical campaigns; fail if >5 newly blocked",
      "urgency": "P1",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_393",
      "area": "Risk",
      "question": "How do we prevent policy agent rule drift when rules are updated in one region but not globally?",
      "why_it_matters": "C18 Red; multi-region deployment requires consistent policy enforcement",
      "evidence_anchor": ["agents/policy_agent.py", "config/policy_rules.yml"],
      "acceptance_signal": "Test: update rule in US→all regions sync <5min; metric policy_sync_lag_seconds",
      "urgency": "P1",
      "owner_hint": "Infra"
    },
    {
      "id": "Q_394",
      "area": "Observability",
      "question": "What is our monitoring agent metric scrape interval and do we alert on scrape failures?",
      "why_it_matters": "C19 Red; missed scrapes create blind spots; current spec lacks scrape health monitoring",
      "evidence_anchor": ["agents/monitoring_agent.py", "config/prometheus_config.yml"],
      "acceptance_signal": "Config: scrape_interval=15s; metric prometheus_scrape_failures_total; alert if >3 consecutive",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_395",
      "area": "Observability",
      "question": "Do we deduplicate monitoring alerts within a time window to prevent alert fatigue?",
      "why_it_matters": "C19 Red; rapid firing of same alert overwhelms ops; grouping required",
      "evidence_anchor": ["agents/monitoring_agent.py", "core/alerting.py"],
      "acceptance_signal": "Config: alert_group_window=5min; test: same alert 3x in 2min→1 notification sent",
      "urgency": "P1",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_396",
      "area": "Compliance",
      "question": "How do we verify audit log immutability after S3 write to prevent tampering?",
      "why_it_matters": "C20 Red; A_015 immutability requirement; S3 versioning insufficient without integrity check",
      "evidence_anchor": ["core/audit.py", "core/s3_writer.py"],
      "acceptance_signal": "Test: write audit→compute HMAC→store HMAC→verify on read; HMAC mismatch→alert",
      "urgency": "P0",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_397",
      "area": "Compliance",
      "question": "What is our audit log retention policy and do we auto-archive after 7 years per compliance?",
      "why_it_matters": "C20 Red; A_015 7yr retention; manual archival error-prone; automation required",
      "evidence_anchor": ["core/audit.py", "config/retention_config.yml"],
      "acceptance_signal": "Config: retention_years=7, auto_archive=true; test: 7yr1d audit→moved to cold storage",
      "urgency": "P1",
      "owner_hint": "Compliance"
    },
    {
      "id": "Q_398",
      "area": "Secrets",
      "question": "Do we rotate audit HMAC keys independently from API keys to isolate breach blast radius?",
      "why_it_matters": "C20 Red; Q_348 KMS isolation; same rotation schedule amplifies compromise risk",
      "evidence_anchor": ["core/audit.py", "core/secrets.py", "config/kms_config.yml"],
      "acceptance_signal": "Config: hmac_kms_id!=api_kms_id, hmac_rotation_days=90, api_rotation_days=30",
      "urgency": "P0",
      "owner_hint": "Secrets"
    },
    {
      "id": "Q_399",
      "area": "Observability",
      "question": "How do we monitor P0 alert delivery latency to PagerDuty to ensure <2min SLA?",
      "why_it_matters": "Monitor Red/CRIT; Q_144 requirement; missed SLA delays incident response",
      "evidence_anchor": ["core/alerting.py", "core/metrics.py"],
      "acceptance_signal": "Metric: pagerduty_delivery_latency_seconds{severity=P0} p95 <120s; alert if >150s",
      "urgency": "P0",
      "owner_hint": "DataOps"
    },
    {
      "id": "Q_400",
      "area": "Infra",
      "question": "What is our circuit breaker trip threshold and do we auto-recover after cooldown period?",
      "why_it_matters": "Infrastructure best practice; Red components lack circuit breakers; prevents cascade failures",
      "evidence_anchor": ["middleware/circuit_breaker.py", "config/resilience_config.yml"],
      "acceptance_signal": "Config: failure_threshold=5, cooldown_seconds=60; test: 5 failures→open, 60s→half-open, 1 success→close",
      "urgency": "P1",
      "owner_hint": "Infra"
    }
  ]
}
